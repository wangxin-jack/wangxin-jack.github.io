<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.62.2" />
  
  
  
  <title>
    
    Hugo Theme Yuki
    
  </title>
  <link rel="canonical" href="https://wangxin-jack.github.io/">
  
  <link rel="alternate" type="application/rss+xml" href="https://wangxin-jack.github.io/index.xml" title="Hugo Theme Yuki">
  
  
  
  
  
  
  































































  
  <link rel="stylesheet" href="https://wangxin-jack.github.io/css/base.min.b7794e36de540988c929f5b91c2250c0ee81952f43bd68f7e234d8e7e128b3ca.css" integrity="sha256-t3lONt5UCYjJKfW5HCJQwO6BlS9DvWj34jTY5&#43;Eos8o=" crossorigin="anonymous">
  
  
</head>
<body>
  <nav class="u-background">
  <div class="u-wrapper">
    <ul class="Banner">
      <li class="Banner-item Banner-item--title">
        <a class="Banner-link u-clickable" href="https://wangxin-jack.github.io/">Hugo Theme Yuki</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://wangxin-jack.github.io/">HOME</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://wangxin-jack.github.io/post">BLOG</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://wangxin-jack.github.io/about">ABOUT</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://wangxin-jack.github.io/tags/">TAGS</a>
      </li>
      
    </ul>
  </div>
</nav>

  <main>
    <div class="u-wrapper">
      <div class="u-padding">
        



<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2018/04/16/using-helm-to-deploy-to-kubernetes/" rel="bookmark">Helm介绍</a>
  </h2>
  
  <time datetime="2018-04-16T15:00:00Z">
    16 April, 2018
  </time>
  
</header>

  
  <h2 id="前言">前言</h2>
<hr>
<p>Helm是Kubernetes生态系统中的一个软件包管理工具。本文将介绍为何要使用Helm进行Kubernetes软件包管理，澄清Helm中使用到的相关概念，并通过一个具体的示例学习如何使用Helm打包，分发，安装，升级及回退Kubernetes应用。</p>
<h2 id="kubernetes应用部署的挑战">Kubernetes应用部署的挑战</h2>
<hr>
<p>让我们首先来看看Kubernetes，kubernetes提供了基于容器的应用集群管理，为容器化应用提供了部署运行、资源调度、服务发现和动态伸缩等一系列完整功能。</p>
<p>kubernetes的核心设计理念是: 用户定义应用程序的规格，而kubernetes则负责按照定义的规则部署并运行应用程序，如果应用系统出现问题导致偏离了定义的规格，kubernetes负责对其进行自动修正。例如应用规格要求部署两个实例，其中一个实例异常终止了，kubernetes会检查到并重新启动一个新的实例。</p>
<p>用户通过使用kubernetes API对象来描述应用程序规格，包括Pod，Service，Volume，Namespace，ReplicaSet，Deployment，Job等等。一般这些对象需要写入一系列的yaml文件中，然后通过kubernetes命令行工具kubectl进行部署。</p>
<p>以下面的wordpress应用程序为例，涉及到多个kubernetes API对象，这些kubernetes API对象分散在多个yaml文件中。</p>
<p>图1： Wordpress应用程序中涉及到的kubernetes API对象
<img src="https://img.zhaohuabing.com/in-post/2018-04-16-using-helm-to-deploy-to-kubernetes/wordpress.png" alt=""></p>
<p>可以看到，在进行kubernetes软件部署时，我们面临下述问题：</p>
<ul>
<li>如何管理，编辑和更新这些这些分散的kubernetes应用配置文件？</li>
<li>如何把一套的相关配置文件作为一个应用进行管理？</li>
<li>如何分发和重用kubernetes的应用配置？</li>
</ul>
<p>Helm的引入很好地解决上面这些问题。</p>
<h2 id="helm是什么">Helm是什么？</h2>
<hr>
<p>很多人都使用过Ubuntu下的ap-get或者CentOS下的yum, 这两者都是Linux系统下的包管理工具。采用apt-get/yum,应用开发者可以管理应用包之间的依赖关系，发布应用；用户则可以以简单的方式查找、安装、升级、卸载应用程序。</p>
<p>我们可以将Helm看作Kubernetes下的apt-get/yum。Helm是Deis (<a href="https://deis.com/">https://deis.com/</a>) 开发的一个用于kubernetes的包管理器。</p>
<p>对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。</p>
<p>对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。</p>
<p>除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。</p>
<h2 id="helm组件及相关术语">Helm组件及相关术语</h2>
<hr>
<p>开始接触Helm时遇到的一个常见问题就是Helm中的一些概念和术语非常让人迷惑，我开始学习Helm就遇到这个问题。</p>
<p>因此我们先了解一下Helm的这些相关概念和术语。</p>
<ul>
<li>
<p>Helm</p>
<p>Kubernetes的应用打包工具，也是命令行工具的名称。</p>
</li>
<li>
<p>Tiller</p>
<p>Helm的服务端，部署在Kubernetes集群中，用于处理Helm的相关命令。</p>
</li>
<li>
<p>Chart</p>
<p>Helm的打包格式，内部包含了一组相关的kubernetes资源。</p>
</li>
<li>
<p>Repoistory</p>
<p>Helm的软件仓库，repository本质上是一个web服务器，该服务器保存了chart软件包以供下载，并有提供一个该repository的chart包的清单文件以供查询。在使用时，Helm可以对接多个不同的Repository。</p>
</li>
<li>
<p>Release</p>
<p>使用Helm install命令在Kubernetes集群中安装的Chart称为Release。</p>
</li>
</ul>
<blockquote>
<p>需要特别注意的是， Helm中提到的Release和我们通常概念中的版本有所不同，这里的Release可以理解为Helm使用Chart包部署的一个应用实例。</p>
<p>其实Helm中的Release叫做Deployment更合适。估计因为Deployment这个概念已经被Kubernetes使用了，因此Helm才采用了Release这个术语。</p>
</blockquote>
<p>下面这张图描述了Helm的几个关键组件Helm（客户端），Tiller（服务器），Repository（Chart软件仓库），Chart（软件包）之前的关系。</p>
<p>图2： Helm软件架构
<img src="https://img.zhaohuabing.com/in-post/2018-04-16-using-helm-to-deploy-to-kubernetes/helm-architecture.png" alt=""></p>
<h2 id="安装helm">安装Helm</h2>
<hr>
<p>下面我们通过一个完整的示例来介绍Helm的相关概念，并学习如何使用Helm打包，分发，安装，升级及回退kubernetes应用。</p>
<p>可以参考Helm的帮助文档https://docs.helm.sh/using_helm/#installing-helm 安装Helm</p>
<p>采用二进制的方式安装Helm</p>
<ol>
<li>下载 Helm <a href="https://github.com/kubernetes/helm/releases">https://github.com/kubernetes/helm/releases</a></li>
<li>解压 tar -zxvf helm-v2.0.0-linux-amd64.tgz</li>
<li>拷贝到bin目录 mv linux-amd64/helm /usr/local/bin/helm</li>
</ol>
<p>然后使用下面的命令安装服务器端组件Tiller</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">Helm init
</code></pre></div><h2 id="构建一个helm-chart">构建一个Helm chart</h2>
<hr>
<p>让我们在实践中来了解Helm。这里将使用一个Go测试小程序，让我们先为这个小程序创建一个Helm chart。</p>
<pre><code>git clone https://github.com/zhaohuabing/testapi.git; 
cd testapi
</code></pre><p>首先创建一个chart的骨架</p>
<pre><code>helm create testapi-chart
</code></pre><p>该命令创建一个testapi-chart目录，该目录结构如下所示，我们主要关注目录中的这三个文件即可: Chart.yaml，values.yaml 和 NOTES.txt。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">testapi-chart
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml
</code></pre></div><ul>
<li>Chart.yaml 用于描述这个chart，包括名字，描述信息以及版本。</li>
<li>values.yaml 用于存储templates目录中模板文件中用到的变量。 模板文件一般是Go模板。如果你需要了解更多关于Go模板的相关信息，可以查看Hugo (<a href="https://gohugo.io">https://gohugo.io</a>) 的一个关于Go模板的介绍 (<a href="https://gohugo.io/templates/go-templates/">https://gohugo.io/templates/go-templates/</a>)。</li>
<li>NOTES.txt 用于向部署该chart的用于介绍chart部署后的一些信息。例如介绍如何使用这个chart，列出缺省的设置等。</li>
</ul>
<p>打开Chart.yaml, 填写你部署的应用的详细信息，以testapi为例：</p>
<pre><code>apiVersion: v1
description: A simple api for testing and debugging
name: testapi-chart
version: 0.0.1
</code></pre><p>然后打开并根据需要编辑values.yaml。下面是testapi应用的values.yaml文件内容。</p>
<pre><code>replicaCount: 2
image:
  repository: daemonza/testapi
  tag: latest
  pullPolicy: IfNotPresent
service:
  name: testapi
  type: ClusterIP
  externalPort: 80
  internalPort: 80
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 128Mi
</code></pre><p>在 testapi_chart 目录下运行下面命令以对chart进行校验。</p>
<pre><code>helm lint
==&gt; Linting .
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, no failures
</code></pre><p>如果文件格式错误，可以根据提示进行修改；如果一切正常，可以使用下面的命令对chart进行打包：</p>
<pre><code>helm package testapi-chart --debug
</code></pre><p>这里添加了 &ndash;debug 参数来查看打包的输出，输出应该类似于：</p>
<pre><code>Saved /Users/daemonza/testapi/testapi-chart/testapi-chart-0.0.1.tgz to current directory
Saved /Users/daemonza/testapi/testapi-chart/testapi-chart-0.0.1.tgz to /Users/daemonza/.helm/repository/local
</code></pre><p>chart被打包为一个压缩包testapi-chart-0.0.1.tgz，该压缩包被放到了当前目录下，并同时被保存到了helm的本地缺省仓库目录中。</p>
<h2 id="helm-repository">Helm Repository</h2>
<hr>
<p>虽然我们已经打包了chart并发布到了helm的本地目录中，但通过Helm search命令查找，并不能找不到刚才生成的chart包。</p>
<pre><code>helm search testapi
No results found
</code></pre><p>这是因为repository目录中的chart还没有被Helm管理。我们可以在本地启动一个Repository Server，并将其加入到Helm repo列表中。</p>
<p>通过helm repo list命令可以看到目前helm中只配置了一个名为stable的repo，该repo指向了google的一个服务器。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm repo list
NAME    URL
stable  https://kubernetes-charts.storage.googleapis.com
</code></pre></div><p>使用helm serve命令启动一个repo server，该server缺省使用'$HELM_HOME/repository/local'目录作为chart存储，并在8879端口上提供服务。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm serve&amp;
Now serving you on 127.0.0.1:8879
</code></pre></div><p>启动本地repo server后，将其加入helm的repo列表。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm repo add local http://127.0.0.1:8879
<span style="color:#e6db74">&#34;local&#34;</span> has been added to your repositories
</code></pre></div><p>现在再查找testapi chart包，就可以找到了。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm search testapi

NAME                    CHART VERSION   APP VERSION     DESCRIPTION
local/testapi-chart     0.0.1                           A Helm chart <span style="color:#66d9ef">for</span> Kubernetes
</code></pre></div><h2 id="在kubernetes中部署chart">在kubernetes中部署Chart</h2>
<hr>
<p>chart被发布到仓储后，可以通过Helm instal命令部署chart，部署时指定chart名及Release（部署的实例）名：</p>
<pre><code> helm install local/testapi-chart --name testapi
</code></pre><p>该命令的输出应类似:</p>
<pre><code>NAME:   testapi
LAST DEPLOYED: Mon Apr 16 10:21:44 2018
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&gt; v1/Service
NAME                   TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)  AGE
testapi-testapi-chart  ClusterIP  10.43.121.84  &lt;none&gt;       80/TCP   0s

==&gt; v1beta1/Deployment
NAME                   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
testapi-testapi-chart  1        1        1           0          0s

==&gt; v1/Pod(related)
NAME                                   READY  STATUS   RESTARTS  AGE
testapi-testapi-chart-9897d9f8c-nn6wd  0/1    Pending  0         0s


NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=testapi-testapi-chart&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)
  echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;
  kubectl port-forward $POD_NAME 8080:80
</code></pre><p>使用下面的命令列出所有已部署的Release以及其对应的Chart。</p>
<pre><code>helm ls
</code></pre><p>该命令的输出应类似:</p>
<pre><code>NAME    REVISION        UPDATED                         STATUS          CHART                   NAMESPACE
testapi 1               Mon Apr 16 10:21:44 2018        DEPLOYED        testapi-chart-0.0.1     default
</code></pre><p>可以看到在输出中有一个Revision（更改历史）字段，该字段用于表示某一Release被更新的次数，可以用该特性对已部署的Release进行回滚。</p>
<h2 id="升级和回退">升级和回退</h2>
<hr>
<p>修改Chart.yaml，将版本号从0.0.1 修改为 1.0.0, 然后使用Helm package命令打包并发布到本地仓库。</p>
<p>查看本地库中的Chart信息，可以看到在本地仓库中testapi-chart有两个版本</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm search testapi -l
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
local/testapi-chart     0.0.1                           A Helm chart <span style="color:#66d9ef">for</span> Kubernetes
local/testapi-chart     1.0.0                           A Helm chart <span style="color:#66d9ef">for</span> Kubernetes
</code></pre></div><p>现在用helm upgrade将已部署的testapi升级到新版本。可以通过参数指定需要升级的版本号，如果没有指定版本号，则缺省使用最新版本。</p>
<pre><code>helm upgrade testapi local/testapi-chart 
</code></pre><p>已部署的testapi release被升级到1.0.0版本</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm list
NAME    REVISION        UPDATED                         STATUS          CHART                   NAMESPACE
testapi <span style="color:#ae81ff">2</span>               Mon Apr <span style="color:#ae81ff">16</span> 10:43:10 <span style="color:#ae81ff">2018</span>        DEPLOYED        testapi-chart-1.0.0     default
</code></pre></div><p>可以通过Helm history查看一个Release的多次更改。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm history testapi
REVISION        UPDATED                         STATUS          CHART                   DESCRIPTION
<span style="color:#ae81ff">1</span>               Mon Apr <span style="color:#ae81ff">16</span> 10:21:44 <span style="color:#ae81ff">2018</span>        SUPERSEDED      testapi-chart-0.0.1     Install complete
<span style="color:#ae81ff">2</span>               Mon Apr <span style="color:#ae81ff">16</span> 10:43:10 <span style="color:#ae81ff">2018</span>        DEPLOYED        testapi-chart-1.0.0     Upgrade complete
</code></pre></div><p>如果更新后的程序由于某些原因运行有问题，我们则需要回退到旧版本的应用，可以采用下面的命令进行回退。其中的参数1是前面Helm history中查看到的Release的更改历史。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm rollback testapi 1
</code></pre></div><p>使用Helm list命令查看，部署的testapi的版本已经回退到0.0.1</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">helm list
NAME    REVISION        UPDATED                         STATUS          CHART                   NAMESPACE
testapi <span style="color:#ae81ff">3</span>               Mon Apr <span style="color:#ae81ff">16</span> 10:48:20 <span style="color:#ae81ff">2018</span>        DEPLOYED        testapi-chart-0.0.1     default
</code></pre></div><h2 id="总结">总结</h2>
<hr>
<p>Helm作为kubernetes应用的包管理以及部署工具，提供了应用打包，发布，版本管理以及部署，升级，回退等功能。Helm以Chart软件包的形式简化Kubernetes的应用管理，提高了对用户的友好性。</p>
<h2 id="qa">Q&amp;A</h2>
<hr>
<p>昨天在Docker.io技术微信群里面进行了Helm的分享，下面是分享过程中得到的一些有意思的反馈，进一步启发了我自己的一些思考。</p>
<p><strong>Q</strong>: Helm结合CD有什么好的建议吗？<!-- raw HTML omitted -->
<strong>A</strong>: 采用Helm可以把零散的Kubernetes应用配置文件作为一个chart管理，chart源码可以和源代码一起放到git库中管理。Helm还简了在CI/CD pipeline的软件部署流程。通过把chart参数化，可以在测试环境和生产环境可以采用不同的chart参数配置。</p>
<p>下图是采用了Helm的一个CI/CD流程
<img src="https://img.zhaohuabing.com/in-post/2018-04-16-using-helm-to-deploy-to-kubernetes/ci-cd-jenkins-helm-k8s.png" alt=""></p>
<p><strong>Q</strong>: 感谢分享，请问下多环境(test,staging，production)的业务配置如何管理呢？通过heml打包configmap吗，比如配置文件更新，也要重新打chats包吗？谢谢，这块我比较乱<!-- raw HTML omitted -->
<strong>A</strong>：Chart是支持参数替换的，可以把业务配置相关的参数设置为模板变量。使用Helm install Chart的时候可以指定一个参数值文件，这样就可以把业务参数从Chart中剥离了。例子： helm install &ndash;values=myvals.yaml wordpress</p>
<p><strong>Q</strong>: helm能解决服务依赖吗？<!-- raw HTML omitted -->
<strong>A</strong>：可以的，在chart可以通过requirements.yaml声明对其他chart的依赖关系。如下面声明表明chart依赖apache和mysql这两个第三方chart。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">dependencies:
  - name: apache
    version: <span style="color:#ae81ff">1.2</span><span style="color:#ae81ff">.3</span>
    repository: http://example.com/charts
  - name: mysql
    version: <span style="color:#ae81ff">3.2</span><span style="color:#ae81ff">.1</span>
    repository: http://another.example.com/charts
</code></pre></div><p><strong>Q</strong>: chart的reversion 可以自定义吗？比如跟git的tag<!-- raw HTML omitted -->
<strong>A</strong>: 这位朋友应该是把chart的version和Release的reversion搞混了，呵呵。 Chart是没有reversion的，Chart部署的一个实例（Release）才有Reversion，Reversion是Release被更新后自动生成的。</p>
<p><strong>Q</strong>: 没有看到helm指向k8s的配置，怎么确认在哪个K8s集群运行的？<!-- raw HTML omitted -->
<strong>A</strong>: 使用和kubectl相同的配置，在  ~/.kube/config 中。</p>
<p><strong>Q</strong>: 这个简单例子并没有看出 Helm 相比 kubectl 有哪些优势，可以简要说一下吗？<!-- raw HTML omitted -->
<strong>A</strong>： Helm将kubernetes应用作为一个软件包整体管理，例如一个应用可能有前端服务器，后端服务器，数据库，这样会涉及多个Kubernetes 部署配置文件，Helm就整体管理了。另外Helm还提供了软件包版本，一键安装，升级，回退。Kubectl和Helm就好比你手工下载安装一个应用 和 使用apt-get 安装一个应用的区别。</p>
<p><strong>Q</strong>: 如何在helm install 时指定命名空间？<!-- raw HTML omitted -->
<strong>A</strong>: helm install local/testapi-chart &ndash;name testapi &ndash;namespace mynamespace</p>
<h2 id="参考">参考</h2>
<hr>
<ul>
<li><a href="https://daemonza.github.io/2017/02/20/using-helm-to-deploy-to-kubernetes/">Using Helm to deploy to Kubernetes</a></li>
<li><a href="https://docs.helm.sh/helm/">Helm documentation</a></li>
<li><a href="https://www.slideshare.net/alexLM/helm-application-deployment-management-for-kubernetes">Helm - Application deployment management for Kubernetes</a></li>
</ul>

  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tech/" rel="tag">Tech</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/kubernetes/" rel="tag">Kubernetes</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/helm/" rel="tag">Helm</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2018/04/11/service-mesh-vs-api-gateway/" rel="bookmark">Service Mesh 和 API Gateway的关系探讨（译文）</a>
  </h2>
  
  <time datetime="2018-04-11T09:32:00Z">
    11 April, 2018
  </time>
  
</header>

  
  <h2 id="service-mesh-vs-api-gateway">Service Mesh vs API Gateway</h2>
<p>在<a href="https://medium.com/microservices-in-practice/service-mesh-for-microservices-2953109a3c9a">前一篇关于Service Mesh的文章</a>中,我提到了几个关于Service Mesh和API Gateway之间关系的问题，在本篇文章中，我打算就Service Mesh和API Gateway的用途进行进一步讨论。</p>
<p>为了区分API Gateway和Service Mesh，让我们先分别看看两者各自的关键特征。</p>
<h2 id="api-gateway-将服务作为被管理的api向外部暴露">API Gateway: 将服务作为被管理的API向外部暴露</h2>
<p>使用API Gateway的主要目的是将微服务作为被管理的API暴露（给外部系统）。因此，我们在API Gateway层开发的API或者边界服务对外提供了业务功能。</p>
<p>API/边界服务调用下游的组合或者原子微服务，通过组合/混装多个下游微服务的方式来提供业务逻辑。</p>
<p>在API/Edge服务调用下游服务时，需要采用一种可靠的通信方式，应用了断路器，超时，负载均衡/故障转移等可靠性模式。因此大部分的API Gateway解决方案都内置了这些特性。</p>
<p>API Gateway也内置了以下特性的支持，包括：服务发现，分析（可见性：性能指标，监控，分布式日志，分布式调用追踪）和安全。</p>
<p>API Gateway和API管理生态系统的其他组件的关系紧密，比如： API 市场/商店， API 发布门户。</p>
<h2 id="service-mesh微服务的网络通信基础设施">Service Mesh：微服务的网络通信基础设施</h2>
<p>现在我们来看看Service Mesh有哪些不同。</p>
<p>Service Mesh是一个网络通信基础设施， 可以用于将应用层的网络通信功能从你的服务代码中剥离出来。</p>
<p>采用Service Mesh， 你不用在服务代码中实现用于可靠通信的模式如断路，超时等，类似地，Service Mesh也提供了服务发现，服务可见性等其他功能。</p>
<h2 id="api-gateway和service-mesh实践">API Gateway和Service Mesh实践</h2>
<p>API Gateway和Service Mesh之间的主要不同点：API Gateway是暴露API/边界服务的关键组件，而Service Mesh则仅仅是一个服务间通信的基础设施，并不了解应用中的业务逻辑。</p>
<p>下图说明了API Gateway和Service Mesh的关系。如同前面所说，这两者之间也有一些重叠的部分（例如断路器等），但重要的是需要理解这两者是用于完全不同的用途。</p>
<p>图1： API Gateway和Service Mesh实践</p>
<p><img src="https://img.zhaohuabing.com/in-post/2018-04-11-service-mesh-vs-api-gateway/service-mesh-vs-api-gateway.png" alt=""></p>
<p>如上图所示，Service Mesh作为Sidecar（边车）和服务一起部署，它是独立于服务的业务逻辑的。</p>
<p>另一方面，API Gateway 提供了所有的API服务（这些API服务有明确定义的业务功能），它是应用业务逻辑的一部分。API Gateway可以具有内建的服务间通信能力，但它也可以使用Service Mesh来调用下游服务（API Gateway-&gt;Service Mesh-&gt;Microservices）。</p>
<p>在API管理层次，你可以使用API Gateway内建的服务间通信能力；也可以通过Service Mesh来调用下游服务，以将应用网络通信功能从应用程序转移到Service Mesh中。</p>
<h2 id="译者按">译者按</h2>
<p>API Gateway和Service Mesh的关系是我最近一直在思考的问题，也和同事及社区的朋友之间进行了一些讨论。这篇短文很清晰地总结了两者之间的相似之处以及这两者在微服务架构中的不同用途。</p>
<p>文章中提到“可以使用API Gateway内建的服务间通信能力；也可以通过Service Mesh来调用下游服务”。在和同事讨论时，大家提到一个比较重要的考虑因素是在API Gateway处引入一个Sidecar可能带来的额外延迟。</p>
<p>API Gateway作为微服务引用的流量入口，其对效率要求较高，如果随API Gateway部署一个Sidecar，可能对效率有一定影响。</p>
<p>我对此未进行测试，但从理论上来说，服务发现，重试，断路等逻辑无论放到API Gateway还是Service Mesh中耗时应该是差不多的，部署Sidecar只是增加了创建一个本地链接的消耗，如下图所示:
<img src="https://img.zhaohuabing.com/in-post/2018-04-11-service-mesh-vs-api-gateway/api-gateway-with-service-mesh.png" alt=""></p>
<p>将API Gateway和Service Mesh的功能进行清晰划分，API Gateway负责应用逻辑，Service Mesh负责服务通讯，Metrics收集等微服务基础设施，这样划分后在架构上更为清晰。对于效率问题，我们可以考虑对API Gateway进行水平扩展来解决。</p>
<h2 id="原文">原文</h2>
<p>本译文发表已征得原作者同意，原文参见 <a href="https://medium.com/microservices-in-practice/service-mesh-vs-api-gateway-a6d814b9bf56">Service Mesh vs API Gateway</a></p>

  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tech/" rel="tag">Tech</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/microservice/" rel="tag">Microservice</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/service-mesh/" rel="tag">Service Mesh</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/api-gateway/" rel="tag">API Gateway</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2018/03/29/what-is-service-mesh-and-istio/" rel="bookmark">谈谈微服务架构中的基础设施：Service Mesh与Istio</a>
  </h2>
  
  <time datetime="2018-03-29T12:00:00Z">
    29 March, 2018
  </time>
  
</header>

  
  <h2 id="微服务架构的演进">微服务架构的演进</h2>
<p>作为一种架构模式，微服务将复杂系统切分为数十乃至上百个小服务，每个服务负责实现一个独立的业务逻辑。这些小服务易于被小型的软件工程师团队所理解和修改，并带来了语言和框架选择灵活性，缩短应用开发上线时间，可根据不同的工作负载和资源要求对服务进行独立缩扩容等优势。</p>
<p>另一方面，当应用被拆分为多个微服务进程后，进程内的方法调用变成了了进程间的远程调用。引入了对大量服务的连接、管理和监控的复杂性。</p>
<p><img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/microservice.PNG" alt=""></p>
<p>该变化带来了分布式系统的一系列问题，例如：</p>
<ul>
<li>如何找到服务的提供方？</li>
<li>如何保证远程方法调用的可靠性？</li>
<li>如何保证服务调用的安全性？</li>
<li>如何降低服务调用的延迟？</li>
<li>如何进行端到端的调试？</li>
</ul>
<p>另外生产部署中的微服务实例也增加了运维的难度,例如：</p>
<ul>
<li>如何收集大量微服务的性能指标已进行分析？</li>
<li>如何在不影响上线业务的情况下对微服务进行升级？</li>
<li>如何测试一个微服务集群部署的容错和稳定性？</li>
</ul>
<p>这些问题涉及到成百上千个服务的通信、管理、部署、版本、安全、故障转移、策略执行、遥测和监控等，要解决这些微服务架构引入的问题并非易事。</p>
<p>让我们来回顾一下微服务架构的发展过程。在出现服务网格之前，我们最开始在微服务应用程序内理服务之间的通讯逻辑，包括服务发现，熔断，重试，超时，加密，限流等逻辑。
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/1.png" alt="">
在一个分布式系统中，这部分逻辑比较复杂，为了为微服务应用提供一个稳定、可靠的基础设施层，避免大家重复造轮子，并减少犯错的可能，一般会通过对这部分负责服务通讯的逻辑进行抽象和归纳，形成一个代码库供各个微服务应用程序使用，如下图所示：
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/2.png" alt="">
公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题：</p>
<ul>
<li>微服务通讯逻辑对应用开发人员并不透明，应用开发人员需要理解并正确使用代码			库，不能将其全部精力聚焦于业务逻辑。</li>
<li>需要针对不同的语言/框架开发不同的代码库，反过来会影响微服务应用开发语言			和框架的选择，影响技术选择的灵活性。</li>
<li>随着时间的变化，代码库会存在不同的版本，不同版本代码库的兼容性和大量运行			环境中微服务的升级将成为一个难题。</li>
</ul>
<p>可以将微服务之间的通讯基础设施层和TCP/IP协议栈进行类比。TCP/IP协议栈为操作系统中的所有应用提供基础通信服务，但TCP/IP协议栈和应用程序之间并没有紧密的耦合关系，应用只需要使用TCP/IP协议提供的底层通讯功能,并不关心TCP/IP协议的实现，如IP如何进行路由，TCP如何创建链接等。</p>
<p>同样地，微服务应用也不应该需要关注服务发现，Load balancing，Retries，Circuit Breaker等微服务之间通信的底层细节。如果将为微服务提供通信服务的这部分逻辑从应用程序进程中抽取出来，作为一个单独的进程进行部署，并将其作为服务间的通信代理，可以得到如下图所示的架构：
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/sidecar.png" alt="">
因为通讯代理进程伴随应用进程一起部署，因此形象地把这种部署方式称为“sidecar”/边车（即三轮摩托的挎斗）。</p>
<p>应用间的所有流量都需要经过代理，由于代理以sidecar方式和应用部署在同一台主机上，应用和代理之间的通讯可以被认为是可靠的。由代理来负责找到目的服务并负责通讯的可靠性和安全等问题。</p>
<p>当服务大量部署时，随着服务部署的sidecar代理之间的连接形成了一个如下图所示的网格，该网格成为了微服务的通讯基础设施层，承载了微服务之间的所有流量，被称之为Service Mesh（服务网格）。
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/mesh.png" alt=""></p>
<p>_服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。</p>
<p>_William Morgan <em><a href="https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/"><em>WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE?</em>
</a></em></p>
<p>服务网格中有数量众多的Sidecar代理，如果对每个代理分别进行设置，工作量将非常巨大。为了更方便地对服务网格中的代理进行统一集中控制，在服务网格上增加了控制面组件。</p>
<p><img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/controlplane.png" alt=""></p>
<p>这里我们可以类比SDN的概念，控制面就类似于SDN网管中的控制器，负责路由策略的指定和路由规则下发；数据面类似于SDN网络中交换机，负责数据包的转发。</p>
<p>由于微服务的所有通讯都由服务网格基础设施层提供，通过控制面板和数据面板的配合，可以对这些通讯进行监控、托管和控制，以实现微服务灰度发布，调用分布式追踪，故障注入模拟测试，动态路由规则，微服务闭环控制等管控功能。</p>
<h2 id="istio服务网格">Istio服务网格</h2>
<p>Istio是一个Service Mesh开源项目，是Google继Kubernetes之后的又一力作，主要参与的公司包括Google，IBM和Lyft。</p>
<p>凭借kubernetes良好的架构设计及其强大的扩展性，Google围绕kubernetes打造一个生态系统。Kubernetes用于微服务的编排（编排是英文Orchestration的直译，用大白话说就是描述一组微服务之间的关联关系，并负责微服务的部署、终止、升级、缩扩容等）。其向下用CNI(容器网络接口），CRI（容器运行时接口）标准接口可以对接不同的网络和容器运行时实现，提供微服务运行的基础设施。向上则用Istio提供了微服务治理功能。</p>
<p>由下图可见，Istio补充了Kubernetes生态圈的重要一环，是Google的微服务版图里一个里程碑式的扩张。
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/k8s-ecosystem.PNG" alt=""></p>
<p>Google借Istio的力量推动微服务治理的事实标准，对Google自身的产品Google Cloud有极其重大的意义。其他的云服务厂商，如Redhat，Pivotal，Nginx，Buoyant等看到大势所趋，也纷纷跟进，宣布自身产品和Istio进行集成，以避免自己被落下，丢失其中的市场机会。</p>
<p>可以预见不久的将来，对于云原生应用而言，采用kubernetes进行服务部署和集群管理，采用Istio处理服务通讯和治理，将成为微服务应用的标准配置。</p>
<p>Istio服务包括网格由数据面和控制面两部分。</p>
<ul>
<li>数据面由一组智能代理（Envoy）组成，代理部署为边车，调解和控制微服务之间所有的网络通信。</li>
<li>控制面负责管理和配置代理来路由流量，以及在运行时执行策略。</li>
</ul>
<p><img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/istio-architecture.png" alt=""></p>
<h3 id="istio控制面">Istio控制面</h3>
<p>Istio控制面板包括3个组件:Pilot, Mixer和Istio-Auth。</p>
<h4 id="pilot">Pilot</h4>
<p>Pilot维护了网格中的服务的标准模型，这个标准模型是独立于各种底层平台的。Pilot通过适配器和各底层平台对接，以填充此标准模型。</p>
<p>例如Pilot中的Kubernetes适配器通过Kubernetes API服务器得到kubernetes中pod注册信息的更改，入口资源以及存储流量管理规则等信息，然后将该数据被翻译为标准模型提供给Pilot使用。通过适配器模式，Pilot还可以从Mesos, Cloud Foundry, Consul中获取服务信息，也可以开发适配器将其他提供服务发现的组件集成到Pilot中。</p>
<p>除此以外，Pilo还定义了一套和数据面通信的标准API，API提供的接口内容包括服务发现 、负载均衡池和路由表的动态更新。通过该标准API将控制面和数据面进行了解耦，简化了设计并提升了跨平台的可移植性。基于该标准API已经实现了多种Sidecar代理和Istio的集成，除Istio目前集成的Envoy外，还可以和Linkerd, Nginmesh等第三方通信代理进行集成，也可以基于该API自己编写Sidecar实现。</p>
<p>Pilot还定义了一套DSL（Domain Specific Language）语言，DSL语言提供了面向业务的高层抽象，可以被运维人员理解和使用。运维人员使用该DSL定义流量规则并下发到Pilot，这些规则被Pilot翻译成数据面的配置，再通过标准API分发到Envoy实例，可以在运行期对微服务的流量进行控制和调整。
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/pilot.png" alt=""></p>
<h4 id="mixer">Mixer</h4>
<p>在微服务应用中，通常需要部署一些基础的后端公共服务以用于支撑业务功能。这些基础设施包括策略类如访问控制，配额管理；以及遥测报告如APM，日志等。微服务应用和这些后端支撑系统之间一般是直接集成的，这导致了应用和基础设置之间的紧密耦合，如果因为运维原因需要对基础设置进行升级或者改动，则需要修改各个微服务的应用代码，反之亦然。</p>
<p>为了解决该问题，Mixer在应用程序代码和基础架构后端之间引入了一个通用中间层。该中间层解耦了应用和后端基础设施，应用程序代码不再将应用程序代码与特定后端集成在一起，而是与Mixer进行相当简单的集成，然后Mixer负责与后端系统连接。</p>
<p>Mixer主要提供了三个核心功能：</p>
<ul>
<li>前提条件检查。允许服务在响应来自服务消费者的传入请求之前验证一些前提条件。前提条件可以包括服务使用者是否被正确认证，是否在服务的白名单上，是否通过ACL检查等等。</li>
<li>配额管理。 使服务能够在分配和释放多个维度上的配额，配额这一简单的资源管理工具可以在服务消费者对有限资源发生争用时，提供相对公平的（竞争手段）。Rate Limiting就是配额的一个例子。</li>
<li>遥测报告。使服务能够上报日志和监控。在未来，它还将启用针对服务运营商以及服务消费者的跟踪和计费流。</li>
</ul>
<p>Mixer的架构如图所示:
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/mixer2.png" alt=""></p>
<p>首先，Sidecar会从每一次请求中收集相关信息，如请求的路径，时间，源IP，目地服务，tracing头，日志等，并请这些属性上报给Mixer。Mixer和后端服务之间是通过适配器进行连接的，Mixer将Sidecar上报的内容通过适配器发送给后端服务。</p>
<p>由于Sidecar只和Mixer进行对接，和后端服务之间并没有耦合，因此使用Mixer适配器机制可以接入不同的后端服务，而不需要修改应用的代码，例如通过不同的Mixer适配器，可以把Metrics收集到Prometheus或者InfluxDB，甚至可以在不停止应用服务的情况下动态切换后台服务。</p>
<p>其次，Sidecar在进行每次请求处理时会通过Mixer进行策略判断，并根据Mixer返回的结果决定是否继续处理该次调用。通过该方式，Mixer将策略决策移出应用层，使运维人员可以在运行期对策略进行配置，动态控制应用的行为，提高了策略控制的灵活性。例如可以配置每个微服务应用的访问白名单，不同客户端的Rate limiting，等等。</p>
<p>逻辑上微服务之间的每一次请求调用都会经过两次Mixer的处理：调用前进行策略判断，调用后进行遥测数据收集。Istio采用了一些机制来避免Mixer的处理影响Envoy的转发效率。</p>
<p>从上图可以看到，Istio在Envoy中增加了一个Mixer Filter，该Filter和控制面的Mixer组件进行通讯，完成策略控制和遥测数据收集功能。Mixer Filter中保存有策略判断所需的数据缓存，因此大部分策略判断在Envoy中就处理了，不需要发送请求到Mixer。另外Envoy收集到的遥测数据会先保存在Envoy的缓存中，每隔一段时间再通过批量的方式上报到Mixer。</p>
<h4 id="auth">Auth</h4>
<p>Istio支持双向SSL认证（Mutual SSL Authentication）和基于角色的访问控制（RBAC），以提供端到端的安全解决方案。</p>
<h5 id="认证">认证</h5>
<p>Istio提供了一个内部的CA(证书机构),该CA为每个服务颁发证书，提供服务间访问的双向SSL身份认证，并进行通信加密，其架构如下图所示：
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/auth.png" alt=""></p>
<p>其工作机制如下：
部署时：</p>
<ul>
<li>CA监听Kubernetes API Server, 为集群中的每一个Service Account创建一对密钥和证书，并发送给Kubernetes API Server。注意这里不是为每个服务生成一个证书，而是为每个Service Account生成一个证书。Service Account和kubernetes中部署的服务可以是一对多的关系。Service Account被保存在证书的SAN(Subject Alternative Name)字段中。</li>
<li>当Pod创建时，Kubernetes根据该Pod关联的Service Account将密钥和证书以Kubernetes Secrets资源的方式加载为Pod的Volume，以供Envoy使用。</li>
<li>Pilot生成数据面的配置，包括Envoy需使用的密钥和证书信息，以及哪个Service Account可以允许运行哪些服务，下发到Envoy。</li>
</ul>
<blockquote>
<p>备注：如果是虚机环境，则采用一个Node Agent生成密钥，向Istio CA申请证书，然后将证书传递给Envoy。</p>
</blockquote>
<p>运行时：</p>
<ul>
<li>服务客户端的出站请求被Envoy接管。</li>
<li>客户端的Envoy和服务端的Envoy开始双向SSL握手。在握手阶段，客户端Envoy会验证服务端Envoy证书中的Service Account有没有权限运行该请求的服务，如没有权限，则认为服务端不可信，不能创建链接。</li>
<li>当加密TSL链接创建好后，请求数据被发送到服务端的Envoy，然后被Envoy通过一个本地的TCP链接发送到服务中。</li>
</ul>
<h5 id="鉴权">鉴权</h5>
<p>Istio“基于角色的访问控制”（RBAC）提供了命名空间，服务，方法三个不同大小粒度的服务访问权限控制。其架构如下图所示：
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/authorization.png" alt=""></p>
<p>管理人员可以定制访问控制的安全策略，这些安全策略保存在Istio Config Store中。 Istio RBAC Engine从Config Store中获取安全策略，根据安全策略对客户端发起的请求进行判断，并返回鉴权结果（允许或者禁止）。</p>
<p>Istio RBAC Engine目前被实现为一个Mixer Adapter，因此其可以从Mixer传递过来的上下文中获取到访问请求者的身份（Subject）和操作请求（Action），并通过Mixer对访问请求进行策略控制，允许或者禁止某一次请求。</p>
<p>Istio Policy中包含两个基本概念：</p>
<ul>
<li>
<p>ServiceRole，定义一个角色，并为该角色指定对网格中服务的访问权限。指定角色访问权限时可以在命名空间，服务，方法的不同粒度进行设置。</p>
</li>
<li>
<p>ServiceRoleBinding，将角色绑定到一个Subject，可以是一个用户，一组用户或者一个服务。</p>
</li>
</ul>
<h3 id="istio数据面">Istio数据面</h3>
<p>Istio数据面以“边车”(sidecar)的方式和微服务一起部署，为微服务提供安全、快速、可靠的服务间通讯。由于Istio的控制面和数据面以标准接口进行交互，因此数据可以有多种实现，Istio缺省使用了Envoy代理的扩展版本。</p>
<p>Envoy是以C ++开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy的许多内置功能被Istio发扬光大，例如动态服务发现，负载均衡，TLS加密，HTTP/2 &amp; gRPC代理，熔断器，路由规则，故障注入和遥测等。</p>
<p>Istio数据面支持的特性如下：</p>
<table>
<thead>
<tr>
<th>Outbound特性</th>
<th>Inbound特性</th>
</tr>
</thead>
<tbody>
<tr>
<td>Service authentication（服务认证）</td>
<td>Service authentication（服务认证）</td>
</tr>
<tr>
<td>Load Balancing（负载均衡）</td>
<td>Authorization（鉴权）</td>
</tr>
<tr>
<td>Retry and circuit breaker（重试和断路器）</td>
<td>Rate limits（请求限流）</td>
</tr>
<tr>
<td>Fine-grained routing（细粒度的路由）</td>
<td>Load shedding（负载控制）</td>
</tr>
<tr>
<td>Telemetry（遥测）</td>
<td>Telemetry（遥测）</td>
</tr>
<tr>
<td>Request Tracing（分布式追踪）</td>
<td>Request Tracing（分布式追踪）</td>
</tr>
<tr>
<td>Fault Injection（故障注入）</td>
<td>Fault Injection（故障注入）</td>
</tr>
</tbody>
</table>
<blockquote>
<p>备注：Outbound特性是指服务请求侧的Sidecar提供的功能特性，而Inbound特性是指服务提供侧Sidecar提供的功能特性。一些特性如遥测和分布式跟踪需要两侧的Sidecar都提供支持；而另一些特性则只需要在一侧提供，例如鉴权只需要在服务提供侧提供，重试只需要在请求侧提供。</p>
</blockquote>
<h3 id="典型应用场景">典型应用场景</h3>
<p>Istio服务管控包括下列的典型应用场景：</p>
<h4 id="分布式调用追踪">分布式调用追踪</h4>
<p>在微服务架构中，业务的调用链非常复杂，一个来自用户的请求可能涉及到几十个服务的协同处理。因此需要一个跟踪系统来记录和分析同一次请求在整个调用链上的相关事件，从而帮助研发和运维人员分析系统瓶颈，快速定位异常和优化调用链路。</p>
<p>Istio通过在Envoy代理上收集调用相关数据，实现了对应用无侵入的分布式调用跟踪分析。 Istio实现分布式调用追踪的原理如下图所示:
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/distributed-tracing.png" alt="">
Envoy收集一个端到端调用中的各个分段的数据，并将这些调用追踪信息发送给Mixer，Mixer Adapter 将追踪信息发送给相应的服务后端进行处理。整个调用追踪信息的生成流程不需要应用程序介入，因此不需要将分布式跟踪相关代码注入到应用程序中。</p>
<blockquote>
<p>注意：应用仍需要在进行出口调用时将收到的入口请求中tracing相关的header转发出去，传递给调用链中下一个边车进行处理。</p>
</blockquote>
<h4 id="度量收集">度量收集</h4>
<p>Istio 实现度量收集的原理如下图所示:
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/metrics-collecting.png" alt=""></p>
<p>Envoy收集指标相关的原始数据，如请求的服务，HTTP状态码，调用时延等，这些收集到的指标数据被送到Mixer，通过Mixer Adapters 将指标信息转换后发送到后端的监控系统中。由于Mixer使用了插件机制，后端监控系统可以根据需要在运行期进行动态切换。</p>
<h4 id="灰度发布">灰度发布</h4>
<p>当应用上线以后，运维面临的一大挑战是如何能够在不影响已上线业务的情况下进行升级。无论进行了多么完善的测试，都无法保证线下测试时发现所有潜在故障。在无法百分百避免版本升级故障的情况下，需要通过一种方式进行可控的版本发布，把故障影响控制在可以接受的范围内，并可以快速回退。</p>
<p>可以通过灰度发布（又名金丝雀发布）来实现业务从老版本到新版本的平滑过渡，并避免升级过程中出现的问题对用户造成的影响。</p>
<p>Istio通过高度的抽象和良好的设计采用一致的方式实现了灰度发布。在发布新版本后，运维人员可以通过定制路由规则将特定的流量（如具有指定特征的测试用户）导入新版本服务中以进行测试。通过渐进受控地向新版本导入生产流量，可以最小化升级中出现的故障对用户的影响。</p>
<p>采用Istio进行灰度发布的流程如下图所示：</p>
<p>首先，通过部署新版本的服务，并将通过路由规则将金丝雀用户的流量导入到新版本服务中
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/canary-1.png" alt=""></p>
<p>测试稳定后，使用路由规则将生产流量逐渐导入到新版本系统中，如按5%，10%，50%，80%逐渐导入。
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/canary-2.png" alt=""></p>
<p>如果新版本工作正常，则最后将所有流量导入到新版本服务中，并将老版本服务下线；如中间出现问题，则可以将流量重新导回老版本，在新版本中修复故障后采用该流程重新发布。
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/canary-3.png" alt=""></p>
<h4 id="断路器">断路器</h4>
<p>在微服务架构中，存在着许许多多的服务单元，若一个服务出现故障，就会因依赖关系形成故障蔓延，最终导致整个系统的瘫痪，这样的架构相较传统架构就更加的不稳定。为了解决这样的问题，因此产生了断路器模式。</p>
<p>断路器模式指，在某个服务发生故障时，断路器的故障监控向调用放返回一个及时的错误响应，而不是长时间的等待。这样就不会使得调用线程因调用故障被长时间占用，从而避免了故障在整个系统中的蔓延。</p>
<p>Istio 实现断路器的原理如下图所示:
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/circuitbreaker.png" alt="">
管理员通过destination policy设置断路触发条件，断路时间等参数。例如设置服务B发生10次5XX错误后断路15分钟。则当服务B的某一实例满足断路条件后，就会被从LB池中移除15分钟。在这段时间内，Envoy将不再把客户端的请求转发到该服务实例。</p>
<p>Istio的断路器还支持配置最大链接数，最大待处理请求数，最大请求数，每链接最大请求数，重试次数等参数。当达到设置的最大请求数后，新发起的请求会被Envoy直接拒绝。
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/circuitbreaker-parameters.png" alt=""></p>
<h4 id="故障注入">故障注入</h4>
<p>对于一个大型微服务应用而言，系统的健壮性非常重要。在微服务系统中存在大量的服务实例，当部分服务实例出现问题时，微服务应用需要具有较高的容错性，通过重试，断路，自愈等手段保证系统能够继续对外正常提供服务。因此在应用发布到生产系统强需要对系统进行充分的健壮性测试。</p>
<p>对微服务应用进行健壮性测试的一个最大的困难是如何对系统故障进行模拟。在一个部署了成百上千微服务的测试环境中，如果想通过对应用，主机或者交换机进行设置来模拟微服务之间的通信故障是非常困难的。</p>
<p>Istio通过服务网格承载了微服务之间的通信流量，因此可以在网格中通过规则进行故障注入，模拟部分微服务出现故障的情况，对整个应用的健壮性进行测试。</p>
<p>故障注入的原理如下图所示：
<img src="https://img.zhaohuabing.com/in-post/2018-03-29-what-is-service-mesh-and-istio/fault-injection.png" alt="">
测试人员通过Pilot向Envoy注入了一个规则，为发向服务MS-B的请求加入了指定时间的延迟。当客户端请求发向MSB-B时，Envoy会根据该规则为该请求加入时延，引起客户的请求超时。通过设置规则注入故障的方式，测试人员可以很方便地模拟微服务之间的各种通信故障，对微服务应用的健壮性进行较为完整的模拟测试。</p>
<h2 id="总结">总结</h2>
<p>服务网格为微服务提供了一个对应用程序透明的安全、可靠的通信基础设施层。采用服务网格后，微服务应用开发人员可以专注于解决业务领域问题，将一些通用问题交给服务网格处理。采用服务网格后，避免了代码库带来的依赖，可以充分发挥微服务的异构优势，开发团队可以根据业务需求和开发人员能力自由选择技术栈。</p>
<p>Istio具有良好的架构设计，提供了强大的二次开发扩展性和用户定制能力。虽然Istio目前还处于beta阶段，但已经获得众多知名公司和产品的支持，是一个非常具有前景的开源服务网格开源项目。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://istio.io/docs/">Istio online documentation</a></li>
<li><a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html">Pattern: Service Mesh</a></li>
<li><a href="https://istio.io/blog/2017/mixer-spof-myth.html">Mixer and the SPOF Myth</a></li>
</ul>
  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tech/" rel="tag">Tech</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/microservice/" rel="tag">Microservice</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/service-mesh/" rel="tag">Service Mesh</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/istio/" rel="tag">Istio</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2018/03/13/use-docker-behind-http-proxy/" rel="bookmark">如何配置docker使用HTTP代理</a>
  </h2>
  
  <time datetime="2018-03-13T18:00:00Z">
    13 March, 2018
  </time>
  
</header>

  
  <h2 id="ubuntu">Ubuntu</h2>
<h3 id="设置docker使用http-proxy">设置docker使用http proxy</h3>
<pre><code>sudo /etc/default/docker

export http_proxy=&quot;http://127.0.0.1:3128/&quot;
export https_proxy=&quot;http://127.0.0.1:3128/&quot;
export HTTP_PROXY=&quot;http://127.0.0.1:3128/&quot;
export HTTPS_PROXY=&quot;http://127.0.0.1:3128/&quot;
</code></pre><h3 id="加载配置并重启docker">加载配置并重启docker</h3>
<pre><code>sudo service docker restart
</code></pre><h2 id="centos">CentOS</h2>
<h3 id="设置docker使用http-proxy-1">设置docker使用http proxy</h3>
<pre><code>sudo mkdir -p /etc/systemd/system/docker.service.d

echo '
[Service]
Environment=&quot;HTTP_PROXY=http://proxy.foo.bar.com:80/&quot;
' | sudo tee /etc/systemd/system/docker.service.d/http-proxy.conf
</code></pre><h3 id="加载配置并重启docker-1">加载配置并重启docker</h3>
<pre><code>sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre>
  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tips/" rel="tag">Tips</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/tips/" rel="tag">Tips</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/docker/" rel="tag">Docker</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2018/02/09/vim-tips/" rel="bookmark">Vim Tips</a>
  </h2>
  
  <time datetime="2018-02-09T11:00:00Z">
    9 February, 2018
  </time>
  
</header>

  
  <h2 id="vim-graphical-cheat-sheet">vim graphical cheat sheet</h2>
<p><img src="/https://img.zhaohuabing.com/in-post/2018-02-09-vim-tips/vi-vim-cheat-sheet.svg" alt=""></p>
<h2 id="vim-jumps">Vim Jumps</h2>
<ul>
<li>^ — Move to start of line</li>
<li>$ — Move to end of line</li>
<li>b — Move back a word</li>
<li>w — Move forward a word</li>
<li>e — Move to the end of the next word</li>
<li>Ctrl-o and Ctrl-i to go to the previous/next location you jumped to</li>
<li>``(two backticks) jump back to where you were</li>
<li>gi go back to the last place you inserted a text and enter insert mode</li>
</ul>
<h2 id="vim-navigations">Vim Navigations</h2>
<ul>
<li>{ and } jump paragraph back and forth</li>
<li>Ctrl-F/B move one screen back and forth</li>
<li>Search the word under cursor, then n/p to jump to next/previous</li>
</ul>
<h2 id="enable-vim-mode-in-bash">Enable Vim mode in bash</h2>
<p>vi ~/.inputrc
set editing-mode vi</p>
<h2 id="enable-system-clipboard-upport">Enable system clipboard upport</h2>
<p>See if system clipboard is supported:</p>
<pre><code>$ vim --version | grep clipboard
-clipboard       +iconv           +path_extra      -toolbar
+eval            +mouse_dec       +startuptime     -xterm_clipboard
</code></pre><p>Rinstall vim as vim-gnome:</p>
<pre><code>sudo apt-get install vim-gnome
</code></pre><p>Select what you want using the mouse - then type to copy to clipboard:</p>
<pre><code>&quot;+y
</code></pre><p>To paste to vim from clipboard type:</p>
<pre><code>&quot;+p
</code></pre><h2 id="others">Others</h2>
<ul>
<li>Ex: open the current directory</li>
<li>set number: show line number</li>
</ul>
  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tips/" rel="tag">Tips</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/tips/" rel="tag">Tips</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/vim/" rel="tag">Vim</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2018/02/09/docker-without-sudo/" rel="bookmark">如何使用非root用户执行docker命令</a>
  </h2>
  
  <time datetime="2018-02-09T10:00:00Z">
    9 February, 2018
  </time>
  
</header>

  
  <h3 id="add-the-docker-group-if-it-doesnt-already-exist">Add the docker group if it doesn't already exist:</h3>
<p>sudo groupadd docker</p>
<h3 id="add-the-connected-user-user-to-the-docker-group-change-the-user-name-to-match-your-preferred-user-if-you-do-not-want-to-use-your-current-user">Add the connected user &ldquo;$USER&rdquo; to the docker group. Change the user name to match your preferred user if you do not want to use your current user:</h3>
<p>sudo gpasswd -a $USER docker</p>
<h3 id="either-do-a-newgrp-docker-or-log-outin-to-activate-the-changes-to-groups">Either do a newgrp docker or log out/in to activate the changes to groups.</h3>

  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tips/" rel="tag">Tips</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/tips/" rel="tag">Tips</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/docker/" rel="tag">Docker</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2018/05/22/user_authentication_authorization/" rel="bookmark">如何构建安全的微服务应用？</a>
  </h2>
  
  <time datetime="2018-02-03T12:00:00Z">
    3 February, 2018
  </time>
  
</header>

  
  <h2 id="前言">前言</h2>
<p>微服务架构的引入为软件应用带来了诸多好处：包括小开发团队，缩短开发周期，语言选择灵活性，增强服务伸缩能力等。与此同时，也引入了分布式系统的诸多复杂问题。其中一个挑战就是如何在微服务架构中实现一个灵活，安全，高效的认证和鉴权方案。本文将尝试就此问题进行一次比较完整的探讨。</p>
<h2 id="单体应用的实现方式">单体应用的实现方式</h2>
<p>在单体架构下，整个应用是一个进程，在应用中，一般会用一个安全模块来实现用户认证和鉴权。</p>
<p>用户登录时，应用的安全模块对用户身份进行验证，验证用户身份合法后，为该用户生成一个会话(Session)，并为该Session关联一个唯一的编号(Session Id)。Session是应用中的一小块内存结构，其中保存了登录用户的信息，如User name, Role, Permission等。服务器把该Session的Session Id返回给客户端，客户端将Session Id以cookie或者URL重写的方式记录下来，并在后续请求中发送给应用，这样应用在接收到客户端访问请求时可以使用Session Id验证用户身份，不用每次请求时都输入用户名和密码进行身份验证。</p>
<blockquote>
<p>备注：为了避免Session Id被第三者截取和盗用，客户端和应用之前应使用TLS加密通信，session也会设置有过期时间。</p>
</blockquote>
<p><img src="https://img.zhaohuabing.com/in-post/2018-02-03-authentication-and-authorization-of-microservice/monolith-user-login.png" alt="单体应用用户登录认证序列图"></p>
<!-- raw HTML omitted -->
<p>客户端访问应用时，Session Id随着HTTP请求发送到应用，客户端请求一般会通过一个拦截器处理所有收到的客户端请求。拦截器首先判断Session Id是否存在，如果该Session Id存在，就知道该用户已经登录。然后再通过查询用户权限判断用户能否执行该此请求，以实现操作鉴权。
<img src="https://img.zhaohuabing.com/in-post/2018-02-03-authentication-and-authorization-of-microservice/monolith-user-request.png" alt="单体应用用户操作鉴权序列图"></p>
<!-- raw HTML omitted -->
<h2 id="微服务认证和鉴权面临的问题">微服务认证和鉴权面临的问题</h2>
<p>在微服务架构下，一个应用被拆分为多个微服务进程，每个微服务实现原来单体应用中一个模块的业务功能。应用拆分后，对每个微服务的访问请求都需要进行认证和鉴权。如果参考单体应用的实现方式会遇到下述问题：</p>
<ul>
<li>认证和鉴权逻辑需要在每个微服务中进行处理，需要在各个微服务中重复实现这部分公共逻辑。虽然我们可以使用代码库复用部分代码，但这又会导致所有微服务对特定代码库及其版本存在依赖，影响微服务语言/框架选择的灵活性。</li>
<li>微服务应遵循单一职责原理，一个微服务只处理单一的业务逻辑。认证和鉴权的公共逻辑不应该放到微服务实现中。</li>
<li>为了充分利用微服务架构的好处，实现微服务的水平扩展(Scalability)和弹性(Resiliency),微服务最好是无状态的。因此不建议使用session这种有状态的方案。</li>
<li>微服务架构下的认证和鉴权涉及到场景更为复杂，涉及到用户访问微服务应用，第三方应用访问微服务应用，应用内多个微服务之间相互访问等多种场景，每种场景下的认证和鉴权方案都需要考虑到，以保证应用程序的安全性。
<img src="https://img.zhaohuabing.com/in-post/2018-02-03-authentication-and-authorization-of-microservice/auth-scenarios.png" alt="微服务认证和鉴权涉及到的三种场景"></li>
</ul>
<!-- raw HTML omitted -->
<h2 id="微服务认证和鉴权的技术方案">微服务认证和鉴权的技术方案</h2>
<h3 id="用户身份认证">用户身份认证</h3>
<p>一个完整的微服务应用是由多个相互独立的微服务进程组成的，对每个微服务的访问都需要进行用户认证。如果将用户认证的工作放到每个微服务中，应用的认证逻辑将会非常复杂。因此需要考虑一个SSO（单点登录）的方案，即用户只需要登录一次，就可以访问所有微服务提供的服务。 由于在微服务架构中以API Gateway作为对外提供服务的入口，因此可以考虑在API Gateway处提供统一的用户认证。</p>
<h3 id="用户状态保持">用户状态保持</h3>
<p>HTTP是一个无状态的协议，对服务器来说，用户的每次HTTP请求是相互独立的。互联网是一个巨大的分布式系统，HTTP协议作为互联网上的一个重要协议，要考虑到大量应用访问的效率问题。无状态意味着服务端可以把客户端的请求根据需要发送到集群中的任何一个节点，HTTP的无状态设计对负载均衡有明显的好处，由于没有状态，用户请求可以被分发到任意一个服务器，应用也可以在靠近用户的网络边缘部署缓存服务器。对于不需要身份认证的服务，例如浏览新闻网页等，这是没有任何问题的。但很多服务如网络购物，企业管理系统等都需要对用户的身份进行认证，因此需要在HTTP协议基础上采用一种方式保存用户的登录状态，避免用户每发起一次请求都需要进行验证。</p>
<p>传统方式是在服务器端采用Cookie来保存用户状态，由于在服务器是有状态的，对服务器的水平扩展有影响。在微服务架构下建议采用Token来记录用户登录状态。</p>
<p>Token和Seesion主要的不同点是存储的地方不同。Session是集中存储在服务器中的；而Token是用户自己持有的，一般以cookie的形式存储在浏览器中。Token中保存了用户的身份信息，每次请求都会发送给服务器，服务器因此可以判断访问者的身份，并判断其对请求的资源有没有访问权限。</p>
<p>Token用于表明用户身份，因此需要对其内容进行加密，避免被请求方或者第三者篡改。<a href="https://jwt.io">JWT(Json Web Token)</a>是一个定义Token格式的开放标准(RFC 7519),定义了Token的内容，加密方式，并提供了各种语言的lib。</p>
<p>JWT Token的结构非常简单，包括三部分：</p>
<ul>
<li>Header<!-- raw HTML omitted -->
头部包含类型,为固定值JWT。然后是JWT使用的Hash算法。</li>
</ul>
<pre><code>{
  &quot;alg&quot;: &quot;HS256&quot;,
  &quot;typ&quot;: &quot;JWT&quot;
}
</code></pre><ul>
<li>Payload<!-- raw HTML omitted -->
包含发布者，过期时间，用户名等标准信息，也可以添加用户角色，用户自定义的信息。</li>
</ul>
<pre><code>{
  &quot;sub&quot;: &quot;1234567890&quot;,
  &quot;name&quot;: &quot;John Doe&quot;,
  &quot;admin&quot;: true
}
</code></pre><ul>
<li>Signature<!-- raw HTML omitted -->
Token颁发方的签名，用于客户端验证Token颁发方的身份，也用于服务器防止Token被篡改。
签名算法</li>
</ul>
<pre><code>HMACSHA256(
  base64UrlEncode(header) + &quot;.&quot; +
  base64UrlEncode(payload),
  secret)
</code></pre><p>这三部分使用Base64编码后组合在一起，成为最终返回给客户端的Token串，每部分之间采用&rdquo;.&ldquo;分隔。下图是上面例子最终形成的Token
<img src="https://cdn.auth0.com/content/jwt/encoded-jwt3.png" alt="">
采用Token进行用户认证，服务器端不再保存用户状态，客户端每次请求时都需要将Token发送到服务器端进行身份验证。Token发送的方式<a href="https://tools.ietf.org/html/rfc6750">rfc6750</a>进行了规定，采用一个 Authorization: Bearer HHTP Header进行发送。</p>
<pre><code>Authorization: Bearer mF_9.B5f-4.1JqM
</code></pre><p>采用Token方式进行用户认证的基本流程如下图所示：</p>
<ol>
<li>用户输入用户名,密码等验证信息，向服务器发起登录请求</li>
<li>服务器端验证用户登录信息，生成JWT token</li>
<li>服务器端将Token返回给客户端，客户端保存在本地（一般以Cookie的方式保存）</li>
<li>客户端向服务器端发送访问请求，请求中携带之前颁发的Token</li>
<li>服务器端验证Token，确认用户的身份和对资源的访问权限，并进行相应的处理（拒绝或者允许访问）
<img src="https://cdn.auth0.com/content/jwt/jwt-diagram.png" alt=""></li>
</ol>
<!-- raw HTML omitted -->
<h3 id="实现单点登录">实现单点登录</h3>
<p>单点登录的理念很简单，即用户只需要登录应用一次，就可以访问应用中所有的微服务。API Gateway提供了客户端访问微服务应用的入口，Token实现了无状态的用户认证。结合这两种技术，可以为微服务应用实现一个单点登录方案。</p>
<p>用户的认证流程和采用Token方式认证的基本流程类似，不同之处是加入了API Gateway作为外部请求的入口。</p>
<p>用户登录</p>
<ol>
<li>客户端发送登录请求到API Gateway</li>
<li>API Gateway将登录请求转发到Security Service</li>
<li>Security Service验证用户身份，并颁发Token</li>
</ol>
<p>用户请求</p>
<ol>
<li>客户端请求发送到API Gateway</li>
<li>API Gateway调用的Security Service对请求中的Token进行验证，检查用户的身份</li>
<li>如果请求中没有Token，Token过期或者Token验证非法，则拒绝用户请求。</li>
<li>Security Service检查用户是否具有该操作权</li>
<li>如果用户具有该操作权限，则把请求发送到后端的Business Service，否则拒绝用户请求
<img src="https://img.zhaohuabing.com/in-post/2018-02-03-authentication-and-authorization-of-microservice/api-gateway-sso.png" alt="采用API Gateway实现微服务应用的SSO"></li>
</ol>
<!-- raw HTML omitted -->
<h3 id="用户权限控制">用户权限控制</h3>
<p>用户权限控制有两种做法，在API Gateway处统一处理，或者在各个微服务中单独处理。</p>
<h4 id="api-gateway处进行统一的权限控制">API Gateway处进行统一的权限控制</h4>
<p>客户端发送的HTTP请求中包含有请求的Resource及HTTP Method。如果系统遵循REST规范，以URI资源方式对访问对象进行建模，则API Gateway可以从请求中直接截取到访问的资源及需要进行的操作，然后调用Security Service进行权限判断，根据判断结果决定用户是否有权限对该资源进行操作，并转发到后端的Business Service。这种实现方式API Gateway处统一处理鉴权逻辑，各个微服务不需要考虑用户鉴权，只需要处理业务逻辑，简化了各微服务的实现。</p>
<h4 id="由各个微服务单独进行权限控制">由各个微服务单独进行权限控制</h4>
<p>如果微服务未严格遵循REST规范对访问对象进行建模，或者应用需要进行定制化的权限控制，则需要在微服务中单独对用户权限进行判断和处理。这种情况下微服务的权限控制更为灵活，但各个微服务需要单独维护用户的授权数据，实现更复杂一些。</p>
<h3 id="第三方应用接入">第三方应用接入</h3>
<p>对于第三方应用接入的访问控制，有两种实现方式：</p>
<h4 id="api-token">API Token</h4>
<p>第三方使用一个应用颁发的API Token对应用的数据进行访问。该Token由用户在应用中生成，并提供给第三方应用使用。在这种情况下，一般只允许第三方应用访问该Token所属用户自身的数据，而不能访问其他用户的敏感私有数据。</p>
<p>例如Github就提供了Personal API Token功能，用户可以在<a href="https://github.com/settings/tokens">Github的开发者设置界面</a>中创建Token，然后使用该Token来访问Github的API。在创建Token时，可以设置该Token可以访问用户的哪些数据，如查看Repo信息，删除Repo，查看用户信息，更新用户信息等。</p>
<p>使用API Token来访问Github API</p>
<pre><code>curl -u zhaohuabing:fbdf8e8862252ed0f3ba9dba4e328c01ac93aeec https://api.github.com/user

</code></pre><p>使用API Token而不是直接使用用户名/密码来访问API的好处是降低了用户密码暴露的风险，并且可以随时收回Token的权限而不用修改密码。</p>
<p>由于API Token只能访问指定用户的数据，因此适合于用户自己开发一些脚本或小程序对应用中自己的数据进行操作。</p>
<h4 id="oauth">OAuth</h4>
<p>某些第三方应用需要访问不同用户的数据，或者对多个用户的数据进行整合处理，则可以考虑采用OAuth。采用OAuth，当第三方应用访问服务时，应用会提示用户授权第三方应用相应的访问权限，根据用户的授权操作结果生成用于访问的Token，以对第三方应用的操作请求进行访问控制。</p>
<p>同样以Github为例，一些第三方应用如Travis CI，GitBook等就是通过OAuth和Github进行集成的。
OAuth针对不同场景有不同的认证流程，一个典型的认证流程如下图所示：</p>
<ul>
<li>用户向OAuth客户端程序发起一个请求，OAuth客户端程序在处理该请求时发现需要访问用户在资源服务器中的数据。</li>
<li>客户端程序将用户请求重定向到认证服务器，该请求中包含一个callback的URL。</li>
<li>认证服务器返回授权页面，要求用户对OAuth客户端的资源请求进行授权。</li>
<li>用户对该操作进行授权后，认证服务器将请求重定向到客户端程序的callback url，将授权码返回给客户端程序。</li>
<li>客户端程序将授权码发送给认证服务器，请求token。</li>
<li>认证服务器验证授权码后将token颁发给客户端程序。</li>
<li>客户端程序采用颁发的token访问资源，完成用户请求。</li>
</ul>
<blockquote>
<p>备注：</p>
<ol>
<li>
<p>OAuth中按照功能区分了资源服务器和认证服务器这两个角色，在实现时这两个角色常常是同一个应用。将该流程图中的各个角色对应到Github的例子中，资源服务器和认证服务器都是Github，客户端程序是Travis CI或者GitBook，用户则是使用Travis CI或者GitBook的直接用户。</p>
</li>
<li>
<p>有人可能会疑惑在该流程中为何要使用一个授权码(Authorization Code)来申请Token，而不是由认证服务器直接返回Token给客户端。OAuth这样设计的原因是在重定向到客户端Callback URL的过程中会经过用户代理（浏览器），如果直接传递Token存在被窃取的风险。采用授权码的方式，申请Token时客户端直接和认证服务器进行交互，并且认证服务期在处理客户端的Token申请请求时还会对客户端进行身份认证，避免其他人伪造客户端身份来使用认证码申请Token。
下面是一个客户端程序采用Authorization Code来申请Token的示例，client_id和client_secret被用来验证客户端的身份。</p>
</li>
</ol>
<pre><code>POST /oauth/token HTTP/1.1
Host: authorization-server.com
 			
grant_type=authorization_code
&amp;code=xxxxxxxxxxx
&amp;redirect_uri=https://example-app.com/redirect
&amp;client_id=xxxxxxxxxx
&amp;client_secret=xxxxxxxxxx
</code></pre></blockquote>
<p><img src="https://img.zhaohuabing.com/in-post/2018-02-03-authentication-and-authorization-of-microservice/oauth_web_server_flow.png" alt="OAuth认证流程"></p>
<!-- raw HTML omitted -->
<p>另外在谈及OAuth时，我们需要注意微服务应用作为OAuth客户端和OAuth服务器的两种不同场景:</p>
<p>在实现微服务自身的用户认证时，也可以采用OAuth将微服务的用户认证委托给一个第三方的认证服务提供商，例如很多应用都将用户登录和微信或者QQ的OAuth服务进行了集成。</p>
<p>第三方应用接入和微服务自身用户认证采用OAuth的目的是不同的，前者是为了将微服务中用户的私有数据访问权限授权给第三方应用，微服务在OAuth架构中是认证和资源服务器的角色；而后者的目的是集成并利用知名认证提供服务商提供的OAuth认证服务，简化繁琐的注册操作，微服务在OAuth架构中是客户端的角色。</p>
<p>因此在我们需要区分这两种不同的场景，以免造成误解。</p>
<h3 id="微服务之间的认证">微服务之间的认证</h3>
<p>除了来自用户和第三方的北向流量外，微服务之间还有大量的东西向流量，这些流量可能在同一个局域网中，也可能跨越不同的数据中心,这些服务间的流量存在被第三方的嗅探和攻击的危险，因此也需要进行安全控制。</p>
<p>通过双向SSL可以实现服务之间的相互身份认证，并通过TLS加密服务间的数据传输。需要为每个服务生成一个证书，服务之间通过彼此的证书进行身份验证。在微服务运行环境中，可能存在大量的微服务实例，并且微服务实例经常会动态变化，例如随着水平扩展增加服务实例。在这种情况下，为每个服务创建并分发证书变得非常困难。我们可以通过创建一个私有的证书中心(Internal PKI/CA)来为各个微服务提供证书管理如颁发、撤销、更新等。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://initiate.andela.com/how-we-solved-authentication-and-authorization-in-our-microservice-architecture-994539d1b6e6">How We Solved Authentication and Authorization in Our Microservice Architecture</a></li>
<li><a href="https://blog.cloudflare.com/how-to-build-your-own-public-key-infrastructure/">How to build your own public key infrastructure</a></li>
<li><a href="https://www.oauth.com/oauth2-servers/access-tokens/authorization-code-request/">OAuth 2.0 Authorization Code Request</a></li>
<li><a href="https://www.jianshu.com/p/c65fa3af1c01">PKI/CA工作原理及架构</a></li>
<li><a href="http://www.primeton.com/read.php?id=2390">深入聊聊微服务架构的身份认证问题</a></li>
</ul>
  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tech/" rel="tag">Tech</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/microservice/" rel="tag">Microservice</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/security/" rel="tag">Security</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2018/01/02/nginmesh-install/" rel="bookmark">Nginx开源Service Mesh组件Nginmesh安装指南</a>
  </h2>
  
  <time datetime="2018-01-02T12:00:00Z">
    2 January, 2018
  </time>
  
</header>

  
  <h2 id="前言">前言</h2>
<p>Nginmesh是NGINX的Service Mesh开源项目，用于Istio服务网格平台中的数据面代理。它旨在提供七层负载均衡和服务路由功能，与Istio集成作为sidecar部署，并将以“标准，可靠和安全的方式”使得服务间通信更容易。Nginmesh在今年底已经连续发布了0.2和0.3版本，提供了服务发现，请求转发，路由规则，性能指标收集等功能。</p>
<p><img src="https://raw.githubusercontent.com/nginmesh/nginmesh/master/images/nginx_sidecar.png" alt="Nginmesh sidecar proxy"></p>
<blockquote>
<p>备注：本文安装指南基于Ubuntu 16.04，在Centos上某些安装步骤的命令可能需要稍作改动。</p>
</blockquote>
<h2 id="安装kubernetes-cluster">安装Kubernetes Cluster</h2>
<p>Kubernetes Cluster包含etcd, api server, scheduler，controller manager等多个组件，组件之间的配置较为复杂，如果要手动去逐个安装及配置各个组件，需要了解kubernetes，操作系统及网络等多方面的知识，对安装人员的能力要求较高。kubeadm提供了一个简便，快速安装Kubernetes Cluster的方式，并且可以通过安装配置文件提供较高的灵活性，因此我们采用kubeadm安装kubernetes cluster。</p>
<p>首先参照<a href="https://kubernetes.io/docs/setup/independent/install-kubeadm">kubeadm的说明文档</a>在计划部署kubernetes cluster的每个节点上安装docker，kubeadm, kubelet 和 kubectl。</p>
<p>安装docker</p>
<pre><code>apt-get update
apt-get install -y docker.io
</code></pre><p>使用google的源安装kubelet kubeadm和kubectl</p>
<pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
</code></pre><p>使用kubeadmin安装kubernetes cluster</p>
<p>Nginmesh使用Kubernetes的<a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/#initializers">Initializer机制</a>来实现sidecar的自动注入。Initializer目前是kubernetes的一个Alpha feature，缺省是未启用的，需要<a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/#enable-initializers-alpha-feature">通过api server的参数</a>打开。因此我们先创建一个kubeadm-conf配置文件，用于配置api server的启动参数</p>
<pre><code>apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
apiServerExtraArgs:
  admission-control: Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ValidatingAdmissionWebhook,ResourceQuota,DefaultTolerationSeconds,MutatingAdmissionWebhook
  runtime-config: admissionregistration.k8s.io/v1alpha1
</code></pre><p>使用kubeadmin init命令创建kubernetes master节点。
可以先试用&ndash;dry-run参数验证一下配置文件。</p>
<pre><code>kubeadm init --config kubeadm-conf --dry-run
</code></pre><p>如果一切正常，kubeadm将提示：Finished dry-running successfully. Above are the resources that would be created.</p>
<p>下面再实际执行创建命令</p>
<pre><code>kubeadm init --config kubeadm-conf
</code></pre><p>kubeadm会花一点时间拉取docker image，命令完成后，会提示如何将一个work node加入cluster。如下所示：</p>
<pre><code> kubeadm join --token fffbf6.13bcb3563428cf23 10.12.5.15:6443 --discovery-token-ca-cert-hash sha256:27ad08b4cd9f02e522334979deaf09e3fae80507afde63acf88892c8b72f143f
</code></pre><blockquote>
<p>备注：目前kubeadm只能支持在一个节点上安装master，支持高可用的安装将在后续版本实现。kubernetes官方给出的workaround建议是定期备份 etcd 数据<a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations">kubeadm limitations</a>。</p>
</blockquote>
<p>Kubeadm并不会安装Pod需要的网络，因此需要手动安装一个Pod网络，这里采用的是Calico</p>
<pre><code>kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml
</code></pre><p>使用kubectl 命令检查master节点安装结果</p>
<pre><code>ubuntu@kube-1:~$ kubectl get all
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
svc/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   12m
</code></pre><p>在每台工作节点上执行上述kubeadm join命令，即可把工作节点加入集群中。使用kubectl 命令检查cluster中的节点情况。</p>
<pre><code> ubuntu@kube-1:~$ kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
kube-1    Ready     master    21m       v1.9.0
kube-2    Ready     &lt;none&gt;    47s       v1.9.0
</code></pre><h2 id="安装istio控制面和bookinfo">安装Istio控制面和Bookinfo</h2>
<p>参考<a href="https://github.com/nginmesh/nginmesh">Nginmesh文档</a>安装Istio控制面和Bookinfo
该文档的步骤清晰明确，这里不再赘述。</p>
<p>需要注意的是，在Niginmesh文档中，建议通过Ingress的External IP访问bookinfo应用程序。但<a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer">Loadbalancer只在支持的云环境中才会生效</a>，并且还需要进行一定的配置。如我在Openstack环境中创建的cluster，则需要参照<a href="https://docs.openstack.org/magnum/ocata/dev/kubernetes-load-balancer.html">该文档</a>对Openstack进行配置后，Openstack才能够支持kubernetes的Loadbalancer service。如未进行配置，通过命令查看Ingress External IP一直显示为pending状态。</p>
<pre><code>NAME            TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE
istio-ingress   LoadBalancer   10.111.158.10   &lt;pending&gt;     80:32765/TCP,443:31969/TCP                                         11m
istio-mixer     ClusterIP      10.107.135.31   &lt;none&gt;        9091/TCP,15004/TCP,9093/TCP,9094/TCP,9102/TCP,9125/UDP,42422/TCP   11m
istio-pilot     ClusterIP      10.111.110.65   &lt;none&gt;        15003/TCP,443/TCP                                                  11m
</code></pre><p>如不能配置云环境提供Loadbalancer特性, 我们可以直接使用集群中的一个节点IP:Nodeport访问Bookinfo应用程序。</p>
<pre><code>http://10.12.5.31:32765/productpage
</code></pre><p>想要了解更多关于如何从集群外部进行访问的内容，可以参考<a href="http://zhaohuabing.com/2017/11/28/access-application-from-outside/">如何从外部访问Kubernetes集群中的应用？</a></p>
<h2 id="查看自动注入的sidecar">查看自动注入的sidecar</h2>
<p>使用 kubectl get pod reviews-v3-5fff595d9b-zsb2q -o yaml 命令查看Bookinfo应用的reviews服务的Pod。</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  annotations:
    sidecar.istio.io/status: injected-version-0.2.12
  creationTimestamp: 2018-01-02T02:33:36Z
  generateName: reviews-v3-5fff595d9b-
  labels:
    app: reviews
    pod-template-hash: &quot;1999151856&quot;
    version: v3
  name: reviews-v3-5fff595d9b-zsb2q
  namespace: default
  ownerReferences:
  - apiVersion: extensions/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: reviews-v3-5fff595d9b
    uid: 5599688c-ef65-11e7-8be6-fa163e160c7d
  resourceVersion: &quot;3757&quot;
  selfLink: /api/v1/namespaces/default/pods/reviews-v3-5fff595d9b-zsb2q
  uid: 559d8c6f-ef65-11e7-8be6-fa163e160c7d
spec:
  containers:
  - image: istio/examples-bookinfo-reviews-v3:0.2.3
    imagePullPolicy: IfNotPresent
    name: reviews
    ports:
    - containerPort: 9080
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-48vxx
      readOnly: true
  - args:
    - proxy
    - sidecar
    - -v
    - &quot;2&quot;
    - --configPath
    - /etc/istio/proxy
    - --binaryPath
    - /usr/local/bin/envoy
    - --serviceCluster
    - reviews
    - --drainDuration
    - 45s
    - --parentShutdownDuration
    - 1m0s
    - --discoveryAddress
    - istio-pilot.istio-system:15003
    - --discoveryRefreshDelay
    - 1s
    - --zipkinAddress
    - zipkin.istio-system:9411
    - --connectTimeout
    - 10s
    - --statsdUdpAddress
    - istio-mixer.istio-system:9125
    - --proxyAdminPort
    - &quot;15000&quot;
    - --controlPlaneAuthPolicy
    - NONE
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: INSTANCE_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    image: nginmesh/proxy_debug:0.2.12
    imagePullPolicy: Always
    name: istio-proxy
    resources: {}
    securityContext:
      privileged: true
      readOnlyRootFilesystem: false
      runAsUser: 1337
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /etc/istio/proxy
      name: istio-envoy
    - mountPath: /etc/certs/
      name: istio-certs
      readOnly: true
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-48vxx
      readOnly: true
  dnsPolicy: ClusterFirst
  initContainers:
  - args:
    - -p
    - &quot;15001&quot;
    - -u
    - &quot;1337&quot;
    image: nginmesh/proxy_init:0.2.12
    imagePullPolicy: Always
    name: istio-init
    resources: {}
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
      privileged: true
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-48vxx
      readOnly: true
  nodeName: kube-2
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - emptyDir:
      medium: Memory
    name: istio-envoy
  - name: istio-certs
    secret:
      defaultMode: 420
      optional: true
      secretName: istio.default
  - name: default-token-48vxx
    secret:
      defaultMode: 420
      secretName: default-token-48vxx
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: 2018-01-02T02:33:54Z
    status: &quot;True&quot;
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: 2018-01-02T02:36:06Z
    status: &quot;True&quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: 2018-01-02T02:33:36Z
    status: &quot;True&quot;
    type: PodScheduled
  containerStatuses:
  - containerID: docker://5d0c189b9dde8e14af4c8065ee5cf007508c0bb2b3c9535598d99dc49f531370
    image: nginmesh/proxy_debug:0.2.12
    imageID: docker-pullable://nginmesh/proxy_debug@sha256:6275934ea3a1ce5592e728717c4973ac704237b06b78966a1d50de3bc9319c71
    lastState: {}
    name: istio-proxy
    ready: true
    restartCount: 0
    state:
      running:
        startedAt: 2018-01-02T02:36:05Z
  - containerID: docker://aba3e114ac1aa87c75e969dcc1b0725696de78d3407c5341691d9db579429f28
    image: istio/examples-bookinfo-reviews-v3:0.2.3
    imageID: docker-pullable://istio/examples-bookinfo-reviews-v3@sha256:6e100e4805a8c10c47040ea7b66f10ad619c7e0068696032546ad3e35ad46570
    lastState: {}
    name: reviews
    ready: true
    restartCount: 0
    state:
      running:
        startedAt: 2018-01-02T02:35:47Z
  hostIP: 10.12.5.31
  initContainerStatuses:
  - containerID: docker://b55108625832a3205a265e8b45e5487df10276d5ae35af572ea4f30583933c1f
    image: nginmesh/proxy_init:0.2.12
    imageID: docker-pullable://nginmesh/proxy_init@sha256:f73b68839f6ac1596d6286ca498e4478b8fcfa834e4884418d23f9f625cbe5f5
    lastState: {}
    name: istio-init
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: docker://b55108625832a3205a265e8b45e5487df10276d5ae35af572ea4f30583933c1f
        exitCode: 0
        finishedAt: 2018-01-02T02:33:53Z
        reason: Completed
        startedAt: 2018-01-02T02:33:53Z
  phase: Running
  podIP: 192.168.79.138
  qosClass: BestEffort
  startTime: 2018-01-02T02:33:39Z

</code></pre><p>该命令行输出的内容相当长，我们可以看到Pod中注入了一个 nginmesh/proxy_debug container,还增加了一个initContainer nginmesh/proxy_init。这两个容器是通过kubernetes initializer自动注入到pod中的。这两个container分别有什么作用呢？让我们看一下<a href="https://github.com/nginmesh/nginmesh/tree/49cd69a61d7d330685ef39ccd63fac06421c3da2/istio/agent">Nginmesh源代码中的说明</a>：</p>
<ul>
<li>
<p>proxy_debug, which comes with the agent and NGINX.</p>
</li>
<li>
<p>proxy_init, which is used for configuring iptables rules for transparently injecting an NGINX proxy from the proxy_debug image into an application pod.</p>
</li>
</ul>
<p>proxy_debug就是sidecar代理，proxy_init则用于配置iptable 规则，以将应用的流量导入到sidecar代理中。</p>
<p>查看proxy_init的Dockerfile文件，可以看到proxy_init其实是调用了<a href="https://github.com/nginmesh/nginmesh/blob/49cd69a61d7d330685ef39ccd63fac06421c3da2/istio/agent/docker-init/prepare_proxy.sh">prepare_proxy.sh</a>这个脚本来创建iptable规则。</p>
<p>proxy_debug Dockerfile</p>
<pre><code>FROM debian:stretch-slim
RUN apt-get update &amp;&amp; apt-get install -y iptables
ADD prepare_proxy.sh /
ENTRYPOINT [&quot;/prepare_proxy.sh&quot;]
</code></pre><p>prepare_proxy.sh节选</p>
<pre><code>...omitted for brevity 

# Create a new chain for redirecting inbound and outbound traffic to
# the common Envoy port.
iptables -t nat -N ISTIO_REDIRECT                                             -m comment --comment &quot;istio/redirect-common-chain&quot;
iptables -t nat -A ISTIO_REDIRECT -p tcp -j REDIRECT --to-port ${ENVOY_PORT}  -m comment --comment &quot;istio/redirect-to-envoy-port&quot;

# Redirect all inbound traffic to Envoy.
iptables -t nat -A PREROUTING -j ISTIO_REDIRECT                               -m comment --comment &quot;istio/install-istio-prerouting&quot;

# Create a new chain for selectively redirecting outbound packets to
# Envoy.
iptables -t nat -N ISTIO_OUTPUT                                               -m comment --comment &quot;istio/common-output-chain&quot;

...omitted for brevity
</code></pre><h2 id="关联阅读">关联阅读</h2>
<p><a href="http://zhaohuabing.com/2017/11/04/istio-install_and_example/">Istio及Bookinfo示例程序安装试用笔记</a></p>
<h2 id="参考">参考</h2>
<ul>
<li>
<p><a href="https://github.com/nginmesh/nginmesh/">Service Mesh with Istio and NGINX</a></p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#14-installing-kubeadm-on-your-hosts">Using kubeadm to Create a Cluster</a></p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/#enable-initializers-alpha-feature">Kubernetes Reference Documentation-Dynamic Admission Control</a></p>
</li>
</ul>
  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tech/" rel="tag">Tech</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/istio/" rel="tag">Istio</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/service-mesh/" rel="tag">service Mesh</a>
    </li>
    
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/nginmesh/" rel="tag">nginmesh</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/post/hellotext/" rel="bookmark">An Example Post</a>
  </h2>
  
  <time datetime="2018-01-01T16:01:23&#43;08:00">
    1 January, 2018
  </time>
  
</header>

  
  <p>hello</p>

  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/two/" rel="tag">two</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/test/" rel="tag">test</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://wangxin-jack.github.io/2017/11/28/access-application-from-outside/" rel="bookmark">如何从外部访问Kubernetes集群中的应用？</a>
  </h2>
  
  <time datetime="2017-11-28T12:00:00Z">
    28 November, 2017
  </time>
  
</header>

  
  <h2 id="前言">前言</h2>
<p>我们知道，kubernetes的Cluster Network属于私有网络，只能在cluster Network内部才能访问部署的应用，那如何才能将Kubernetes集群中的应用暴露到外部网络，为外部用户提供服务呢？本文探讨了从外部网络访问kubernetes cluster中应用的几种实现方式。</p>
<blockquote>
<p>本文尽量试着写得比较容易理解，但要做到“深入浅出”，把复杂的事情用通俗易懂的语言描述出来是非常需要功力的，个人自认尚未达到此境界，唯有不断努力。此外，kubernetes本身是一个比较复杂的系统，无法在本文中详细解释涉及的所有相关概念，否则就可能脱离了文章的主题，因此假设阅读此文之前读者对kubernetes的基本概念如docker，container，pod已有所了解。</p>
</blockquote>
<p>另外此文中的一些内容是自己的理解，由于个人的知识范围有限，可能有误，如果读者对文章中的内容有疑问或者勘误，欢迎大家指证。</p>
<h2 id="pod和service">Pod和Service</h2>
<p>我们首先来了解一下Kubernetes中的Pod和Service的概念。</p>
<p>Pod(容器组),英文中Pod是豆荚的意思，从名字的含义可以看出，Pod是一组有依赖关系的容器，Pod包含的容器都会运行在同一个host节点上，共享相同的volumes和network namespace空间。Kubernetes以Pod为基本操作单元，可以同时启动多个相同的pod用于failover或者load balance。</p>
<p><img src="https://img.zhaohuabing.com/in-post/access-application-from-outside/pod.PNG" alt="Pod"></p>
<p>Pod的生命周期是短暂的，Kubernetes根据应用的配置，会对Pod进行创建，销毁，根据监控指标进行缩扩容。kubernetes在创建Pod时可以选择集群中的任何一台空闲的Host，因此其网络地址是不固定的。由于Pod的这一特点，一般不建议直接通过Pod的地址去访问应用。</p>
<p>为了解决访问Pod不方便直接访问的问题，Kubernetes采用了Service的概念，Service是对后端提供服务的一组Pod的抽象，Service会绑定到一个固定的虚拟IP上，该虚拟IP只在Kubernetes Cluster中可见，但其实该IP并不对应一个虚拟或者物理设备，而只是IPtable中的规则，然后再通过IPtable将服务请求路由到后端的Pod中。通过这种方式，可以确保服务消费者可以稳定地访问Pod提供的服务，而不用关心Pod的创建、删除、迁移等变化以及如何用一组Pod来进行负载均衡。</p>
<p>Service的机制如下图所示，Kube-proxy监听kubernetes master增加和删除Service以及Endpoint的消息，对于每一个Service，kube proxy创建相应的iptables规则，将发送到Service Cluster IP的流量转发到Service后端提供服务的Pod的相应端口上。
<img src="https://img.zhaohuabing.com/in-post/access-application-from-outside/services-iptables-overview.png" alt="Pod和Service的关系"></p>
<blockquote>
<p>备注：虽然可以通过Service的Cluster IP和服务端口访问到后端Pod提供的服务，但该Cluster IP是Ping不通的，原因是Cluster IP只是iptable中的规则，并不对应到一个网络设备。</p>
</blockquote>
<h2 id="service的类型">Service的类型</h2>
<p>Service的类型(ServiceType)决定了Service如何对外提供服务，根据类型不同，服务可以只在Kubernetes cluster中可见，也可以暴露到Cluster外部。Service有三种类型，ClusterIP，NodePort和LoadBalancer。其中ClusterIP是Service的缺省类型，这种类型的服务会提供一个只能在Cluster内才能访问的虚拟IP，其实现机制如上面一节所述。</p>
<h2 id="通过nodeport提供外部访问入口">通过NodePort提供外部访问入口</h2>
<p>通过将Service的类型设置为NodePort，可以在Cluster中的主机上通过一个指定端口暴露服务。注意通过Cluster中每台主机上的该指定端口都可以访问到该服务，发送到该主机端口的请求会被kubernetes路由到提供服务的Pod上。采用这种服务类型，可以在kubernetes cluster网络外通过主机IP：端口的方式访问到服务。</p>
<blockquote>
<p>注意：官方文档中说明了Kubernetes clusterIp的流量转发到后端Pod有Iptable和kube proxy两种方式。但对Nodeport如何转发流量却语焉不详。该图来自网络，从图来看是通过kube proxy转发的，我没有去研究过源码。欢迎了解的同学跟帖说明。</p>
</blockquote>
<p><img src="https://img.zhaohuabing.com/in-post/access-application-from-outside/nodeport.PNG" alt="Pod和Service的关系"></p>
<p>下面是通过NodePort向外暴露服务的一个例子，注意可以指定一个nodePort，也可以不指定。在不指定的情况下，kubernetes会从可用的端口范围内自动分配一个随机端口。</p>
<pre><code>kind: Service
apiVersion: v1
metadata:
  name: influxdb
spec:
  type: NodePort
  ports:
    - port: 8086
      nodePort: 30000
  selector:
    name: influxdb
</code></pre><p>通过NodePort从外部访问有下面的一些问题，自己玩玩或者进行测试时可以使用该方案，但不适宜用于生产环境。</p>
<ul>
<li>
<p>Kubernetes cluster host的IP必须是一个well-known IP，即客户端必须知道该IP。但Cluster中的host是被作为资源池看待的，可以增加删除，每个host的IP一般也是动态分配的，因此并不能认为host IP对客户端而言是well-known IP。</p>
</li>
<li>
<p>客户端访问某一个固定的host IP存在单点故障。假如一台host宕机了，kubernetes cluster会把应用 reload到另一节点上，但客户端就无法通过该host的nodeport访问应用了。</p>
</li>
<li>
<p>该方案假设客户端可以访问Kubernetes host所在网络。在生产环境中，客户端和Kubernetes host网络可能是隔离的。例如客户端可能是公网中的一个手机APP，是无法直接访问host所在的私有网络的。</p>
</li>
</ul>
<p>因此，需要通过一个网关来将外部客户端的请求导入到Cluster中的应用中，在kubernetes中，这个网关是一个4层的load balancer。</p>
<h2 id="通过load-balancer提供外部访问入口">通过Load Balancer提供外部访问入口</h2>
<p>通过将Service的类型设置为LoadBalancer，可以为Service创建一个外部Load Balancer。Kubernetes的文档中声明该Service类型需要云服务提供商的支持，其实这里只是在Kubernetes配置文件中提出了一个要求，即为该Service创建Load Balancer，至于如何创建则是由Google Cloud或Amazon Cloud等云服务商提供的，创建的Load Balancer不在Kubernetes Cluster的管理范围中。kubernetes 1.6版本中，WS, Azure, CloudStack, GCE and OpenStack等云提供商已经可以为Kubernetes提供Load Balancer.下面是一个Load balancer类型的Service例子：</p>
<pre><code>kind: Service
apiVersion: v1
metadata:
  name: influxdb
spec:
  type: LoadBalancer
  ports:
    - port: 8086
  selector:
    name: influxdb
</code></pre><p>部署该Service后，我们来看一下Kubernetes创建的内容</p>
<pre><code>$ kubectl get svc influxdb
NAME       CLUSTER-IP     EXTERNAL-IP     PORT(S)          AGE
influxdb   10.97.121.42   10.13.242.236   8086:30051/TCP   39s
</code></pre><p>Kubernetes首先为influxdb创建了一个集群内部可以访问的ClusterIP 10.97.121.42。由于没有指定nodeport端口，kubernetes选择了一个空闲的30051主机端口将service暴露在主机的网络上，然后通知cloud provider创建了一个load balancer，上面输出中的EEXTERNAL-IP就是load balancer的IP。</p>
<p>测试使用的Cloud Provider是OpenStack，我们通过neutron lb-vip-show可以查看创建的Load Balancer详细信息。</p>
<pre><code>$ neutron lb-vip-show 9bf2a580-2ba4-4494-93fd-9b6969c55ac3
+---------------------+--------------------------------------------------------------+
| Field               | Value                                                        |
+---------------------+--------------------------------------------------------------+
| address             | 10.13.242.236                                                |
| admin_state_up      | True                                                         |
| connection_limit    | -1                                                           |
| description         | Kubernetes external service a6ffa4dadf99711e68ea2fa163e0b082 |
| id                  | 9bf2a580-2ba4-4494-93fd-9b6969c55ac3                         |
| name                | a6ffa4dadf99711e68ea2fa163e0b082                             |
| pool_id             | 392917a6-ed61-4924-acb2-026cd4181755                         |
| port_id             | e450b80b-6da1-4b31-a008-280abdc6400b                         |
| protocol            | TCP                                                          |
| protocol_port       | 8086                                                         |
| session_persistence |                                                              |
| status              | ACTIVE                                                       |
| status_description  |                                                              |
| subnet_id           | 73f8eb91-90cf-42f4-85d0-dcff44077313                         |
| tenant_id           | 4d68886fea6e45b0bc2e05cd302cccb9                             |
+---------------------+--------------------------------------------------------------+

$ neutron lb-pool-show 392917a6-ed61-4924-acb2-026cd4181755
+------------------------+--------------------------------------+
| Field                  | Value                                |
+------------------------+--------------------------------------+
| admin_state_up         | True                                 |
| description            |                                      |
| health_monitors        |                                      |
| health_monitors_status |                                      |
| id                     | 392917a6-ed61-4924-acb2-026cd4181755 |
| lb_method              | ROUND_ROBIN                          |
| members                | d0825cc2-46a3-43bd-af82-e9d8f1f85299 |
|                        | 3f73d3bb-bc40-478d-8d0e-df05cdfb9734 |
| name                   | a6ffa4dadf99711e68ea2fa163e0b082     |
| protocol               | TCP                                  |
| provider               | haproxy                              |
| status                 | ACTIVE                               |
| status_description     |                                      |
| subnet_id              | 73f8eb91-90cf-42f4-85d0-dcff44077313 |
| tenant_id              | 4d68886fea6e45b0bc2e05cd302cccb9     |
| vip_id                 | 9bf2a580-2ba4-4494-93fd-9b6969c55ac3 |
+------------------------+--------------------------------------+

$ neutron lb-member-list
+--------------------------------------+--------------+---------------+--------+----------------+--------+
| id                                   | address      | protocol_port | weight | admin_state_up | status |
+--------------------------------------+--------------+---------------+--------+----------------+--------+
| 3f73d3bb-bc40-478d-8d0e-df05cdfb9734 | 10.13.241.89 |         30051 |      1 | True           | ACTIVE |
| d0825cc2-46a3-43bd-af82-e9d8f1f85299 | 10.13.241.10 |         30051 |      1 | True           | ACTIVE |
+--------------------------------------+--------------+---------------+--------+----------------+--------
</code></pre><p>可以看到OpenStack使用VIP 10.13.242.236在端口8086创建了一个Load Balancer，Load Balancer对应的Lb pool里面有两个成员10.13.241.89 和 10.13.241.10，正是Kubernetes的host节点，进入Load balancer流量被分发到这两个节点对应的Service Nodeport 30051上。</p>
<p>但是如果客户端不在Openstack Neutron的私有子网上，则还需要在load balancer的VIP上关联一个floating IP，以使外部客户端可以连接到load balancer。</p>
<p>部署Load balancer后，应用的拓扑结构如下图所示（注：本图假设Kubernetes Cluster部署在Openstack私有云上）。
<img src="https://img.zhaohuabing.com/in-post/access-application-from-outside/load-balancer.PNG" alt="外部Load balancer"></p>
<blockquote>
<p>备注：如果kubernetes环境在Public Cloud上，Loadbalancer类型的Service创建出的外部Load Balancer已经带有公网IP地址，是可以直接从外部网络进行访问的，不需要绑定floating IP这个步骤。例如在AWS上创建的Elastic Load Balancing (ELB)，有兴趣可以看一下这篇文章：<a href="http://docs.heptio.com/content/tutorials/aws-qs-services-elb.html">Expose Services on your AWS Quick Start Kubernetes cluster</a>。</p>
</blockquote>
<p>如果Kubernetes Cluster是在不支持LoadBalancer特性的cloud provider或者裸机上创建的，可以实现LoadBalancer类型的Service吗？应该也是可以的。Kubernetes本身并不直接支持Loadbalancer，但我们可以通过对Kubernetes进行扩展来实现，可以监听kubernetes Master的service创建消息，并根据消息部署相应的Load Balancer（如Nginx或者HAProxy），来实现Load balancer类型的Service。</p>
<p>通过设置Service类型提供的是四层Load Balancer，当只需要向外暴露一个服务的时候，可以直接采用这种方式。但在一个应用需要对外提供多个服务时，采用该方式会为每一个服务（IP+Port）都创建一个外部load balancer。如下图所示
<img src="https://img.zhaohuabing.com/in-post/access-application-from-outside/multiple-load-balancer.PNG" alt="创建多个Load balancer暴露应用的多个服务">
一般来说，同一个应用的多个服务/资源会放在同一个域名下，在这种情况下，创建多个Load balancer是完全没有必要的，反而带来了额外的开销和管理成本。直接将服务暴露给外部用户也会导致了前端和后端的耦合，影响了后端架构的灵活性，如果以后由于业务需求对服务进行调整会直接影响到客户端。可以通过使用Kubernetes Ingress进行L7 load balancing来解决该问题。</p>
<h2 id="采用ingress作为七层load-balancer">采用Ingress作为七层load balancer</h2>
<p>首先看一下引入Ingress后的应用拓扑示意图（注：本图假设Kubernetes Cluster部署在Openstack私有云上）。
<img src="https://img.zhaohuabing.com/in-post/access-application-from-outside/ingress.PNG" alt="采用Ingress暴露应用的多个服务">
这里Ingress起到了七层负载均衡器和Http方向代理的作用，可以根据不同的url把入口流量分发到不同的后端Service。外部客户端只看到foo.bar.com这个服务器，屏蔽了内部多个Service的实现方式。采用这种方式，简化了客户端的访问，并增加了后端实现和部署的灵活性，可以在不影响客户端的情况下对后端的服务部署进行调整。</p>
<p>下面是Kubernetes Ingress配置文件的示例，在虚拟主机foot.bar.com下面定义了两个Path，其中/foo被分发到后端服务s1，/bar被分发到后端服务s2。</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        backend:
          serviceName: s1
          servicePort: 80
      - path: /bar
        backend:
          serviceName: s2
          servicePort: 80
</code></pre><p>注意这里Ingress只描述了一个虚拟主机路径分发的要求，可以定义多个Ingress，描述不同的7层分发要求，而这些要求需要由一个Ingress Controller来实现。Ingress Contorller会监听Kubernetes Master得到Ingress的定义，并根据Ingress的定义对一个7层代理进行相应的配置，以实现Ingress定义中要求的虚拟主机和路径分发规则。Ingress Controller有多种实现，Kubernetes提供了一个<a href="https://github.com/kubernetes/ingress-nginx">基于Nginx的Ingress Controller</a>。需要注意的是，在部署Kubernetes集群时并不会缺省部署Ingress Controller，需要我们自行部署。</p>
<p>下面是部署Nginx Ingress Controller的配置文件示例，注意这里为Nginx Ingress Controller定义了一个LoadBalancer类型的Service，以为Ingress Controller提供一个外部可以访问的公网IP。</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: LoadBalancer
  ports:
    - port: 80
      name: http
    - port: 443
      name: https
  selector:
    k8s-app: nginx-ingress-lb
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 2
  revisionHistoryLimit: 3
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      containers:
        - name: nginx-ingress-controller
          image: gcr.io/google_containers/nginx-ingress-controller:0.8.3
          imagePullPolicy: Always
    //----omitted for brevity----
</code></pre><blockquote>
<p>备注：Google Cloud直接支持Ingress资源，如果应用部署在Google Cloud中，Google Cloud会自动为Ingress资源创建一个7层load balancer，并为之分配一个外部IP，不需要自行部署Ingress Controller。</p>
</blockquote>
<h2 id="结论">结论</h2>
<p>采用Ingress加上Load balancer的方式可以将Kubernetes Cluster中的应用服务暴露给外部客户端。这种方式比较灵活，基本可以满足大部分应用的需要。但如果需要在入口处提供更强大的功能，如有更高的效率要求，需求进行安全认证，日志记录，或者需要一些应用的定制逻辑，则需要考虑采用微服务架构中的API Gateway模式，采用一个更强大的API Gateway来作为应用的流量入口。</p>
<h2 id="参考">参考</h2>
<ul>
<li>
<p><a href="http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/">Accessing Kubernetes Pods from Outside of the Cluster</a></p>
</li>
<li>
<p><a href="https://daemonza.github.io/2017/02/13/kubernetes-nginx-ingress-controller/">Kubernetes nginx-ingress-controller</a></p>
</li>
<li>
<p><a href="https://docs.openstack.org/magnum/ocata/dev/kubernetes-load-balancer.html">Using Kubernetes external load balancer feature</a></p>
</li>
<li>
<p><a href="http://docs.heptio.com/content/tutorials/aws-qs-services-elb.html">Expose Services on your AWS Quick Start Kubernetes cluster</a></p>
</li>
</ul>
  
  






<footer>
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/categories/tech/" rel="tag">Tech</a>
    </li>
    
    
  </ul>
  
  
  
  <ul class="Tags">
    
    
    <li class="Tags-item u-background">
      <a class="Tags-link u-clickable" href="https://wangxin-jack.github.io/tags/kubernetes/" rel="tag">Kubernetes</a>
    </li>
    
    
  </ul>
  
  
</footer>


</article>
<div class="Divider"></div>


<nav>
  
  <a class="Pagination u-clickable" href="https://wangxin-jack.github.io/page/3/" rel="prev">« Previous</a>
  
  
  <a class="Pagination Pagination--right u-clickable" href="https://wangxin-jack.github.io/" rel="next">Next »</a>
  
</nav>




      </div>
    </div>
  </main>
  
<footer class="Footer">
  <div class="u-wrapper">
    <div class="u-padding">
      Except where otherwise noted, content on this site is licensed under a a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license"> Creative Commons Attribution 4.0 International License</a>.
    </div>
  </div>
</footer>


</body>
</html>
